---
aliases: 
tags: 
type: evergreen
---

# fallacy of incomplete evidence

_previous note:_ [[cognitive biases]]

Cherry picking -- a form of [[confirmation bias]] -- is also known as the fallacy of incomplete evidence, and in science it is an epistemological characteristic of denialism. 

Human beings are cherry pickers. We seek and notice examples that confirm our beliefs or ideas and don't notice or ignore examples that contradict these beliefs. Such confirmation biases are not conscious. For example, goal-oriented motivated reasoning is driven by personal emotions. That is, the heuristic for judging new information is, more or less: how does this information make me feel? 

Underpinning any epistemic system are our beliefs. However ... 

> arguments that threaten our core beliefs and our sense of belonging to a group (e.g., political beliefs) typically trigger all kinds of motivated reasoning (Taber & Lodge 2006; Kahan 2017) whereby one outright rejects those arguments without properly engaging with their content.[@novaes-2021-argument n.pag]

What would it take to change your position on the value of somatics?

> arguments that threaten our core beliefs and our sense of belonging to a group (e.g., political beliefs) typically trigger all kinds of motivated reasoning (Taber & Lodge 2006; Kahan 2017) whereby one outright rejects those arguments without properly engaging with their content. Relatedly, when choosing among a vast supply of options, people tend to gravitate towards content and sources that confirm their existing opinions, thus giving rise to so-called “echo chambers” and “epistemic bubbles” (Nguyen 2020).<https://plato.stanford.edu/entries/argument/#Fall>

- Ellerton article for The Conversation re epistemology and truth is useful. https://read.readwise.io/new/read/01gyhnwb6edbv3zxwh6j4wx4p4 -- very very good as overview of some suggestions I might make. [[Peter Ellerton - How Do You Know That What You Know Is True That’s Epistemology]]

> The sincerity of one’s belief, the volume or frequency with which it is stated, or assurances to “believe me” should not be rationally persuasive by themselves.[@ellerton-2017-how]

Also _Argumentation as an epistemic practice_ -- relates to Nordquist above (but compare with confirmation bias, and the backfire effect! -- motivated reasoning):

> In particular, arguments that threaten our core beliefs and our sense of belonging to a group (e.g., political beliefs) typically trigger all kinds of motivated reasoning (Taber & Lodge 2006; Kahan 2017) whereby one outright rejects those arguments without properly engaging with their content. Relatedly, when choosing among a vast supply of options, people tend to gravitate towards content and sources that confirm their existing opinions, thus giving rise to so-called “echo chambers” and “epistemic bubbles” (Nguyen 2020). Furthermore, some arguments can be deceptively convincing in that they look valid but are not (Tindale 2007; see entry on [fallacies](https://plato.stanford.edu/entries/fallacies/)). Because most of us are arguably not very good at spotting fallacious arguments, especially if they are arguments that lend support to the beliefs we already hold, engaging in argumentation may in fact _decrease_ the accuracy of our beliefs by persuading us of false conclusions with incorrect arguments (Fantl 2018). <https://plato.stanford.edu/entries/argument/#Fall>

They become truisms. A word like that? 

- seems like these are closer to the level of myths or beliefs. Even hopes, wishes or desires. 

Thinking through Ginot's statement that the purpose of science in somatics is to "foster belief" [[somatic practices, interoception and evidence#^fd4fbc]]. 

---

_original source/found:_ [[somatics unlimited edit bin]]

_reference:_ 



