---
aliases: 
tags: 
type: evergreen
---

# Bayes and abductive reasoning

_previous note:_ 

> But it is Bayes whose name is forever tied to a way of reasoning called ‘inference to the best explanation’, the insights from which are central to understanding how conscious perceptions are built from brain-based best guesses. [@seth-2021-being Loc 1615]

> Induction involves reaching conclusions through extrapolating from a series of observations: the sun has risen in the east for all of recorded history, therefore it always rises in the east. 98Unlike deductive inferences, inductive inferences can be wrong: the first three balls I pulled out of the bag were green, therefore all balls in the bag are green. This may or may not be true. (Location 1622)

> Abductive reasoning – the sort formalised by Bayesian inference – is all about finding the best explanation for a set of observations, when these observations are incomplete, uncertain, or otherwise ambiguous. (Location 1625)

> In seeking the ‘best explanation’, abductive reasoning can be thought of as reasoning backward, from observed effects to their most likely causes, rather than forward, from causes to their effects – as is the case for deduction and induction. (Location 1627)

> Did it rain overnight? Perhaps, but it could also be that you forgot to turn off your garden sprinkler. The aim is to find the best explanation, or hypothesis, for what you see: given the lawn is wet, what is the probability (i) that it rained overnight, or (ii) that you left the sprinkler on? In other words, we want to infer the most likely cause for the observed data. (Location 1630)

> Bayes’ rule is a mathematical recipe for going from what we already know (the prior) to what we should believe next (the posterior), based on what we are learning now (the likelihood). Priors, likelihoods, and posteriors are often called Bayesian ‘beliefs’ because they represent states of knowledge rather than states of the world. (Location 1633)

> Bayes’ rule combines priors and likelihoods to come up with posterior probabilities for each hypothesis. The rule itself is simple: the posterior is just the prior multiplied by the likelihood, and divided by a second prior (this is the ‘prior on the data’ – which in this case is the prior probability of a wet lawn – we don’t need to worry about this here since it is the same for each hypothesis). (Location 1644)

> In the philosophy of science, the Bayesian perspective has most in common with the views of the Hungarian philosopher Imre Lakatos, whose analysis focuses on what makes scientific research programmes work in practice, rather than on what they might ideally consist of. (Location 1674)

> For example, I’m aware that I have a strong prior belief that brains are Bayesian-like prediction machines. This strong belief will not only shape how I interpret experimental evidence, it will also determine the sorts of experiments that I do, to generate new evidence relevant to my beliefs. Sometimes I wonder how much evidence it would take to overturn my Bayesian belief that the brain is essentially Bayesian. (Location 1677)

---

_original source/found:_ 

_reference:_ Seth, A. (2021) _Being You: A New Science of Consciousness_. London: Faber & Faber



