---
aliases: []
tags: []
type: "evergreen"
---

# lucy suchman C-DaRE invites discussion

_previous note:_ 

## introduction

- me: C-DaRE; expertise in choreographic practice, improvisation and research centred around practice. NOT AI or ML or LLMs. 
- welcome Lucy Suchman to C-DaRE invites. Lucy is an anthropologist working at the intersections of the fields of anthropology and science and technology studies, and who has written extensively on human-like machines and human understanding of being with machines. Lucy has also looked critically at AI systems in the military in which "human life is reduced to sensor data and machine processing".  -- pleasure to e-meet you. 
- we have prepared by sending back and forth things to read and look at. But this conversation itself is not planned so let's see. 
- acknowledge just how much is being spoken and written about "AI" at the moment and how overwhelming it is (or that it might feel). Because this is a dance research event, we will do our best to stay close to ideas that relate broadly speaking to the body in motion, but forgive us if stray off course
- missed anything Lucy? 

## to the body

- what is your relationship to your body? (engage or not with who or what that possessive pronoun 'your' refers to) -- or how would you describe your relationship to your body?

## the body and subjects and objects

- am academically ambivalent about reification of the human body (and the body in motion) that we in dance tend towards (for obvious reasons of it being our bread and butter, our lens on the world). whatever the question the answer is "the body" (like cognitive scientists and 'dopamine' or computer scientists and 'deep neural networks')

- sport: body as machine -- more or less a system of sub-dividable parts: levers, axes, ropes, pulleys; also mind as a machine: optimised through sports psych. But my proposition is that much dance training is akin to this: optimising or mastering the body as an object to be controlled, coordinated, displayed, and also as a container for expressive potential. Perhaps even in somatic dance practices this sense of the body as observed from the inside (observed by whom?) remains hard to shake. More akin to mindful witness looking at contents
- what does AI (perhaps with respect to robotic systems) have to tell us about the subject-object divide -- that is, that there is an "I" (a 'self' as subject) observing, manipulating, calculating, sensing an object or objects (including the body itself whether machinic or biological)? 
- noting here that this divide is fundamental to infant development -- that there are objects in the world that are graspable, able to be manipulated and moved, and oriented towards (etc)

## definitions and intelligence

Lucy:
> ‘AI’ is a cover term for a range of technologies of data processing and techniques of data analysis based on the iterative adjustment of relevant parameters, according to some combination of internally and externally generated feedback. - Lucy Suchman, 2020 "Comparing domains of improvisation"

Lucy 2:
> I treat the term ‘AI’ as a figure to emphasize that more than a specific technology, or even set of technologies, AI has become what anthropologist Lévi-Strauss (1987) first called a ‘floating signifier’ (p.21 Imaginaries)

Report: 
> The lack of a precise, universally accepted definition of AI probably has helped the field to grow, blossom, and advance at an ever-accelerating pace. - One Hundred Year Study on AI, 2016 report (in Mitchell)

Nick Bostrom definition of AI (in Superintelligence): 
> Accordingly, one can view artificial intelligence as a quest to find shortcuts: ways of tractably approximating the Bayesian ideal by sacrificing some optimality or generality while preserving enough to get high performance in the actual domains of interest. (Location 425)

Kate Crawford (in Atlas of AI):

> “artificial intelligence” is a term that “is both used and rejected in ways that keep its meaning in flux.” She notes, too, that “‘machine learning’ is more commonly used in the technical literature.” Consequently, she chooses to “use AI to talk about the massive industrial formation that includes politics, labor, culture, and capital” while using “machine learning” to refer to “a range of technical approaches.” https://theconvivialsociety.substack.com/p/apocalyptic-ai

Marvin Minsky (one of 'big four' founders of AI back in the 50s) called intelligence a suitcase word. 
John McCarthy and others at Dartmouth conference in 56 came up with phrase 'artificial intelligence' -- McCarthy didn't like the word artificial, but had to call it something.

- what does the word intelligence mean in the context of "AI" and what do you understand intelligence to mean? (I'm really sorry about these questions)
- why not artificial wisdom or understanding or care or common good or responsibility? Why did the boffins back in 1950s choose intelligence?

## being special

> we need to take a careful look at some of the crucial abilities underlying our distinctive human intelligence, such as perception, language, decision-making, commonsense reasoning, and learning.  [@mitchell-2019-artificial Location 1048]

When in your work you talk about humans as subjects what do you mean by "human"? That is, what is it to be human? Are we special? 

(cf Alan Jacobs who is worried about the declining sense of humans being special creatures)

## embodiment

> “[Marvin] Minsky described the human brain as nothing more than a ‘meat machine’ and regarded the body, that ‘bloody mess of organic matter,’ as a ‘teleoperator for the brain.’ Both, he insisted, were eminently replaceable by machinery. What is important about life, Minsky argued, is ‘mind,’ which he defined in terms of ‘structure and subroutines’—that is, programming.” ([View Highlight](https://read.readwise.io/read/01gxdg26cwyn63ca9xhhv4ea7m)) - https://theconvivialsociety.substack.com/p/apocalyptic-ai

I like to think of embodiment as the recognition of an illusion, and the illusion is that it is possible for me to leave my body and I would still be me. Embodiment is the _body being known_.

In 2012, Andrej Karpathy (former senior director of AI at Tesla and whose bio says "I like to train deep neural nets on large datasets") wrote a blog post titled "The state of Computer Vision and AI: we are really, really far away".

> A seemingly inescapable conclusion for me is that we may … need embodiment, and that the only way to build computers that can interpret scenes like we do is to allow them to get exposed to all the years of (structured, temporally coherent) experience we have, ability to interact with the world, and some magical active learning/inference architecture that I can barely even imagine when I think backwards about what it should be capable of. [Karpathy, cited in @mitchell-2019-artificial Location 4269]

> Lakoff and Johnson’s thesis is that not only is our everyday language absolutely teeming with metaphors that are often invisible to us, but our understanding of essentially all abstract concepts comes about via metaphors based on core physical knowledge. [@mitchell-2019-artificial Location 3855]

Assuming this is accurate -- or even to spend time entertaining its accuracy -- how might our sense of the construction or development of 'intelligence' shift/expand from 'brain' (as equivalent to mind) to something in which human intelligence is embedded in the world and afforded through action/movement? Can you imagine what might cause such a shift? In relation to the development of AI, we seem really really far away ...

## scale

Re 'singular body in motion' (my thought here is not about rampant individualism, or the celebration of the individual) but simply that, as a matter of direct experience, the closer each of us pays attention (to the stream of consciousness) the more detail (paradoxes, confusions, possibilities) we see. The less clear they become. 
The Bottomless Well of Infinity -- from the Waking Up app.
https://dynamic.wakingup.com/course/CD3C65 (1 min). 

Anatomy of an AI system by Kate Crawford and Vladan Joler. https://anatomyof.ai

From Crawford and Joler:

> ‘In this fleeting moment of interaction, a vast matrix of capacities is invoked: interlaced chains of resource extraction, human labor and algorithmic processing across networks of mining, logistics, distribution, prediction and optimization. The scale of this system is almost beyond human imagining. How can we begin to see it, to grasp its immensity and complexity as a connected form?’

- The scale. The scale of the body. Even in extreme it can't get close to scale of deep neural networks (imcomprehensible -- even to those who deploy the networks)

- Two parts to this: 
	+ AI and implausibly large data sets that are abstractions even to those who deploy/wield them cf the apparent agential singular body in motion 
	+ "vast matrix of capacities" - a la Crawford and Joler (put into chat)

Learned helplessness as consequence of abstraction. Question of opacity and scale. 

## technologies and human values

Ruthanna Emrys: A Half Built Garden (cli-fi) -- climate-fiction

> I thought again about the dandelion networks. Encoded in algorithms and input interfaces, their root was a set of ideas: that everyone brought worthwhile perception and insight to the decisions that shaped society, that our technologies embodied our values, that they should be consciously designed to do so. And, also, that resilient systems could thrive and grow between cracks, in the face of all opposition. That was hard to remember, when they’d cracked through the concrete and turned it to meadows before I was born. (Location 5606)

-- that technologies embodied our values. I don't know who the 'our' is in that sentence -- perhaps the common good of human kind -- but could you talk to human values in relation to AI?

## being in motion and drawing boundaries (exclusion)

- sensation (at level of direct experience) is in flux. e.g. pay attention to heat, tingling, pressure in hand and these direct experiences (pre-conceptual) are in constant flux, whereas the hand (as we name it) is a concept and is immutable

> The uncertainty that scientific and technical knowledge systems are always dealing with is an uncertainty around boundaries: what counts as ‘this’ and what counts as ‘that’. AI itself is fundamentally in the business of drawing boundaries, of deciding what is included and what is excluded. [@mcquillan-2022-resisting Location 1857}

- do you have some thoughts on exclusion/inclusion re AI and the mutability of direct expereince? When does 'this' body become 'that' body?

## activity is concrete and embodied

> In the pages that follow, however, I argue that all activity, even the most analytic, is fundamentally concrete and embodied. 

- what do you mean by concrete and embodied? 
- what of actions that are not instrumental? There is no purpose other than being in direct experience. Nothing is gained, nothing is lost. No objective is reached. It's in the word objective: striving towards the object. 

- raises questions of in/determinacy -- the lure or perhaps illusion of indeterminacy in human actions (improvisation being the most startling in which we talk the talk of indeterminacy)

## recipes and improvisation

- make a custard
> So I’m thinking about improvisation here in the somewhat unconventional sense not of the alternative to ‘executing’ a plan or procedure, but as what is actually required to do that; the improvisation necessary for accountable action. We could take the favourite analogy in computer science, which compares an algorithm to a recipe. How can we recognise the difference between cooking according to a recipe and inventing a new dish, and/but also the necessary improvisation presupposed by the recipe? This has implications for grasping both the determinate relation of code to computational processes, and the irremediable indeterminacies and contingencies of computing in and as social practice.

- make a custard (touches more on Ingold's work re craft and deeply understood practices that are learned without learning or knowing that one is learning)

- what does Lucy mean by computing in and as social practice? 

## a mutually intelligible world

> it’s helpful to articulate differences in the resources available to people and machines in constructing a mutually intelligible world.  Articulating those differences suggests the value of a shift from treating machines as almost or quasi humans, to recognizing their particular, machinic agencies. - Suchman, 2020

- what might a mutually intelligible world look like? Not least because of how AI seems to have entered the cultural imagination with quite a bang in the last 6-12 months -- you described it even then as a 'frenzy'
- what are their particular machinic agencies? 

## the improbable, novelty and brittleness

- That current ML systems (including chatGPT built on so-called unsupervised learning like LLMs) do not handle novelty. 

> While scientists certainly seek theories that have a high degree of empirical corroboration, as the philosopher Karl Popper noted, “we do not seek highly probable theories but explanations; that is to say, powerful and highly improbable theories.” ([View Highlight](https://read.readwise.io/read/01gw2fek7mhqx1j16t33r877v2)) -- Chomsky (in NY Times)
 
- also the value of the improbable (say in making art or in improvisation). How do you talk and think about novelty when it comes to ML -- the distinction between something on the fringe (outlier in statistical sense) and something not yet existing?

“All of this makes it more unlikely every day,” Arendt warned, “that man will encounter anything in the world around him that is not man-made and hence is not, in the last analysis, he himself in a different disguise.” -- Sacasas The Virtue of Noticing (https://comment.org/the-virtue-of-noticing/)

## time

> Experience isn’t merely the best teacher; it’s the only teacher. [...] there are no shortcuts; if you want to create the common sense that comes from twenty years of being in the world, you need to devote twenty years to the task. You can’t assemble an equivalent collection of heuristics in less time; experience is algorithmically incompressible. (Jenny Odell -- from Ted Chiang Location 2874)
+++++ 
- Ted Chiang: "The lifecycle of software objects". 

> The late Leanne Payne, a teacher of the spiritual life, once said, “We either contemplate or we exploit.” Exploitation asks, What can this person (or for that matter, this thing) do for me? Contemplation asks, Who or what am I beholding, without regard for their usefulness to me?
-- Andy Crouch, The life we're looking for: reclaiming relationship in a technological world

- question about The time of the body ... versus the time of the machine 

Ezra Klein:
> There was the difficulty of living in exponential time, the impossible task of speeding policy and social change to match the rate of viral replication. I suspect that some of the political and social damage we still carry from the pandemic reflects that impossible acceleration. There is a natural pace to human deliberation. A lot breaks when we are denied the luxury of time.
<https://www.nytimes.com/2023/03/12/opinion/chatbots-artificial-intelligence-future-weirdness.html>

## situational awareness

In "Imaginaries of omniscience: Automating intelligence in the US Department of Defense" (2022) you discuss situational awareness (and the OODA loop) regarding closed and open worlds. Could you talk about what these terms 'closed' and 'open' mean and why they matter to bodies? (perhaps OODA loop as well?)

> As [Haraway] pointed out when talking about the concept of situated knowledge, ‘objectivity cannot be about fixed vision when what counts as an object is precisely what world history turns out to be about’ (Haraway, 1988). [@mcquillan-2022-resisting Location 1852]

## improvisation (and recipes)

- Marie Boyce: make a custard

In the writing on improvisation you shared, ask how we could recognise the difference between cooking according to a recipe and inventing a new dish, and/but also the necessary improvisation presupposed by the recipe?

You then write:

> This has implications for grasping both the determinate relation of code to computational processes, and the irremediable indeterminacies and contingencies of computing in and as social practice.

- make a custard (touches more on Ingold's work re craft and deeply understood practices that are learned without learning or knowing that one is learning)

- what do you mean by computing in and as social practice? 

## old and new technologies

Neil Postman: Technopoly: The Surrender of Culture to Technology (1992):

> Surrounding every technology are institutions whose organization–not to mention their reason for being–reflects the world-view promoted by the technology. Therefore, when an old technology is assaulted by a new one, institutions are threatened. When institutions are threatened, a culture finds itself in crisis.

- when you think about AI what is the old technology (c.f. publishing industry post-internet), and what role do we humans (as bodies in motion) have to play in this technology-institution-culture confluence -- and is this confluence even responsive to such bodies? 
- Arendt (Prologue to Human Condition): "the ‘truths’ of the modern scientific worldview, though they can be demonstrated in mathematical formulas and proved technologically, will no longer lend themselves to normal expression in speech and thought."
- this question comes from my reading of L Michael Sacasas's Convivial Society blog: https://theconvivialsociety.substack.com/p/language-under-digital-conditions 
- perhaps my question here is about the seductive lure of these technologies and how easily they compel us to adjust our cultural attitudes. I think I'm adopting a conservative position here -- in sense of wanting to wait, to slow down.

## humans as prediction engines

Annaka Harris mentions cognitive neuroscientist Anil Seth's idea that experiences of self are a form of 'controlled hallucination':

> Seth refers to our experiences of ourselves in the world as a kind of “controlled hallucination.” He describes the brain as a “prediction engine” and explains that “what we perceive is its best guess of what’s out there in the world.” In a sense, he says, “we predict ourselves into existence.”14 [@harris-2019-conscious Location 511]

Seth: British professor of Cognitive and Computational Neuroscience at the University of Sussex (disputes existence of the hard problem of human consciousness -- that we experience things (things that start as neural procecces in brain))

- curious just how much this is akin to AI



## spare questions

- noting that there's an extraordinary amount of noise, resources, opinions, data ... re AI ... what hope is there for the smallness of the sensing human? The line between demographic data (or more specifically epidemiological data - vital in reducing impact of disease in human and other populations) - and their value (perhaps writ large in context of AI) -- and the lived experience of the individual (embedded in community). Where is this line for you? Does such a line exist? 
- Mechanical Turks: still used in self-supervised learning (as in LLMs)? -- at 2nd and 3rd stages? 
- while reading and preparing, I was struck by the sense of strong threads running through your work. As if, even at its most diverse or different, it seems to fit together. Does it feel that way to you? How might adhoc-ness (situated actions) vs (say) the arbitrary nature of curiosity (novelty) have played parts as you look back at these threads? (or perhaps you are not someone who is interested in looking back). And are their implications for technologised bodies under the grip of what Paul Kingsnorth calls 'the machine' of this capacity to make sense looking back? to narrativise that which was merely responsive -- improvising a life even? 
- fear of AI because it tampers with this sense of human agency and control. That we are agential selves.
- what do you see as the responsibilities of a niche -- fringe of the fringe -- discipline like dance and somatic research? 

### useful language

- brittle
- adversarial examples
- Exploration vs exploration (in reinforcement learning)
- indeterminacy

# edit bin

Pattern recognition. Choreography. The pleasure. The desire to see patterns. 

Accounting for practices that are not focused on optimisation. That maybe even resist the gathering of raw materials (data) that ai models require for optimisation. That the practice is deep in the mess of being itself for its own sake. 
There is nothing to predict. Seeking understanding. Awareness? See loss function and "the optimiser" in resisting AI - section on optimisation. 
What if, for example, there is no pattern to be discovered or found? (Eg in improvisation) 


- When do these bodies (flesh, blood etc) become those bodies (bodies of the state?). See latest Sacasas in obsidian -- meat and mince. 

> Below is part of an exchange between the (under-appreciated) philosopher Hans Jonas and Arendt:
>  > **Jonas** : I share with Hannah Arendt the position that we are not in possession of any ultimates, either by knowledge or by conviction or faith […]
>  > 
>  > However, a part of wisdom is knowledge of ignorance. The Socratic attitude is to know that one does not know. And this realization of our ignorance can be of great practical importance in the exercise of the power of judgment, which is after all related to action in the political sphere, into future action, and far-reaching action. ([View Highlight](https://read.readwise.io/read/01gxdggqs641ap3fjmsrmmdbaa)) https://theconvivialsociety.substack.com/p/apocalyptic-ai

> I assert that the fundamental mode of learning of human beings is experiential. Book learning is a layer on top of that.… If human knowledge, especially knowledge about experience, is largely tacit, i.e., never directly and explicitly expressed, it will not be found in books, and the Kurzweil approach to knowledge acquisition will fail.… It is not in what the computer knows but what the computer does not know and cannot know wherein the problem resides.47 [@mitchell-2019-artificial Location 1025]

From Morton's All Art is Ecological

> The aesthetic experience isn’t really about data – it’s about data-ness, the qualities we experience when we apprehend something. (As I mentioned earlier, data just means ‘what is given’, and isn’t only about numbers and pie charts.) The aesthetic experience is about solidarity with what is given. It’s a solidarity, a feeling of alreadiness, for no reason in particular, with no agenda in particular – like evolution, like the biosphere. There is no good reason to distinguish between nonhumans that are ‘natural’ and ones that are ‘artificial’, by which we mean made by humans. It just becomes too difficult to sustain such distinctions. Since, therefore, an artwork is itself a nonhuman being, this solidarity in the artistic realm is already solidarity with nonhumans, whether or not art is explicitly ecological. Ecologically explicit art is simply art that brings this solidarity with the nonhuman to the foreground.

What is intelligence vs understanding vs direct experience 
- What does it mean to understand something? (And how is it different from knowing)
to know and to understand -- what do these terms mean to you?


> In what follows, I’m using the term “AI” in a manner similar to how Kate Crawford uses it in *[Atlas of AI](https://substack.com/redirect/6131b471-0325-44ff-a1c5-cf0aa897534b?j=eyJ1IjoiYXZ6eDQifQ.G0OEO2hYU5EfmDn6Y1N-lMJfqyCMC6azYH_trtWPtnc)* . Crawford observes that “artificial intelligence” is a term that “is both used and rejected in ways that keep its meaning in flux.” She notes, too, that “‘machine learning’ is more commonly used in the technical literature.” Consequently, she chooses to “use AI to talk about the massive industrial formation that includes politics, labor, culture, and capital” while using “machine learning” to refer to “a range of technical approaches.” Likewise, I am using AI here not to designate an array of specific technical practice and capabilities, but rather the present amorphous techno-cultural idea of AI as it is deployed, debated, feared, and celebrated. This idea of AI is not only as a massive industrial formation including politics, labor, culture, and capital, but also the consummation of a historical development which we ordinarily gloss as modernity. ([View Highlight](https://read.readwise.io/read/01gxdfx65xhbcav2bmb932wryd)) -- Sacasas: https://theconvivialsociety.substack.com/p/apocalyptic-ai

Mandy Brown is a technology blogger and in a post called "Smoke screen" she writes:

> Stories about machines that learn or achieve something like intelligence serve to dress up what the machines can do, to make something as basic as what amounts to a very expensive autocomplete seem like toddlers preordained to become gods or dictators.

https://aworkinglibrary.com/writing/smoke-screen



> This definition is not meant to understate the extraordinary scale and complexity of the multi-dimensional models and the layered architectures of so-called machine or ‘deep’ learning systems, or the exponentially growing speed and storage capacity of the hardware that enables them. But it is meant to underscore that these are elaborations of pattern identification based not on significance (or learning) in the human sense, but on computationally detectable correlations that, however meaningless, selectively produce results that are legible to humans. From training data to the assessment of results, it is humans who inform the input and find significance in the output of an algorithmic system’s operations. 

> Ursula Le Guin defined technology as “what we can learn to do”. She also defined technology very broadly – a refrigerator, a computer, a fishhook, a pair of shoes – as “the active human interface with the material world.” Anything you can learn to do builds your capacity, creates agency, puts you in charge of your own life and makes it meaningful. Only then can hope settle, and be put to use.
- https://web.archive.org/web/20220614152007/http://www.ursulakleguinarchive.com/Note-Technology.html
- from James Bridle: http://booktwo.org/notebook/hope-needs-a-place-to-perch/

Or put differently. The statement "I know my body" -- ask "known by what?" there is nothing to find. Instead, embodiment is more like _body being known_

> I am generally convinced by those who, in trying to describe the modern mode of relating to the world, find that it is best characterized as a relation of mastery and control, which is to say a relation of power. Positioned in this way, we are tempted to see the world, including ourselves, as a field of objects to be endlessly manipulated, optimized, and exploited. The world appears to us chiefly as raw material for our own projects. It is there primarily to be used and transformed to serve our purposes. ([View Highlight](https://read.readwise.io/read/01gpd9gsmq6natfx5wb0nfxrrs))
- Note: Suchman
- Sacasas: On Two Ways of Relating to the World - https://theconvivialsociety.substack.com/p/on-two-ways-of-relating-to-the-world


Sacasas The Virtue of Noticing (https://comment.org/the-virtue-of-noticing/)
> Writing in the early days of manned space exploration, Hannah Arendt noted that recent progress in science, once translated into everyday life, “has brought with it a veritable avalanche of fabulous instruments and ever more ingenious machinery,” which might evoke a certain kind of amazement, but an amazement ultimately at our own ingenuity. “All of this makes it more unlikely every day,” Arendt warned, “that man will encounter anything in the world around him that is not man-made and hence is not, in the last analysis, he himself in a different disguise.” ([View Highlight](https://read.readwise.io/read/01gphe7ye5bsh0qgspse25fvad))
> Or, as the playwright Max Frisch once quipped, technology is “the knack of so arranging the world that we don’t have to experience it.” ([View Highlight](https://read.readwise.io/read/01gphe9zkffgbyea2mbx9kh9xg))
- Note: Suchman
- about nature and the choices we make??? 






Abstraction. That something conceived of as data is an abstraction. A short hand. A proxy. Dancing is not abstract. 
Critical shift. Reduction. Different from continuity. Bergson. Analogue. 
Whitehead: "explaining away" - abstractions as something concrete. Lossy and lossless. 
Heidegger: engraving. Technology shaping the way the world is revealed. Standing reserve. 

Borgmann: focal 

> SEEING MORE OF the world as constitutive of time, full of agency, and deserving of respect means abandoning that hierarchy that Tinker mentions, between the actor and the acted upon. Is this exhilarating or fearsome? Wildcat writes that “indigenous thinkers not only acknowledge contingency and human’s lack of control in the world; they also see it as empowering and humbling, not something frightening.” If “empowering and humbling” sounds like a paradox, it’s because of how we normally conceive of power. In a worldview where power, agency, and experience are not bound by individual bodies but reside “in and through the relations and processes that constitute life,” the paradox dissolves. (Odell Saving Time Location 2889)


+++++ 
- Note: Suchman -- the actor and the acted upon. them and us. 
- "not bound by individual bodies" -- beautiful

> A review of How to Do Nothing once said that I "employ[ed] the annoying term 'bodies' " when, clearly, I must have meant people or humans. But I don't mean "people" or "humans." I mean bodies: double bodies, triple bodies, alliances and amalgamations that can shift and bear the weight, brace the walls. (Location 3542) - Odell 'saving time'

- what might we mean by bodies and what limitations or possibilities are there worth exploring or speaking to directly? 

What is and isn't able to be expressed in code. 

The phrase "my animal body". Or animal body. That we (humans) are animals. 


something about the discourse of inevitability re AI. Something this person (Shannon) rails against: https://read.readwise.io/new/read/01gr79kk52hx2abzz2739bkv67 

> I’m talking about machine learning. Which is itself one kind of story—one in which machines do something like “learn,” but which really means to memorize or put into storage, and includes nothing so pedestrian as understanding or interpreting. But the more common parlance—“artificial intelligence”—expands on that story to suggest that not only are the machines learning, but they have acquired the ability to *think*, or to intellectualize, implying that they have desires and personalities and behaviors. One way this story works is that by ascribing “thinking” to the machines, it triggers associations many people have with “higher” beings—whether species that are smarter than others, or people that are. (I’ll come back to this hierarchical notion of intelligence in a moment.)

> to make something as basic as what amounts to a very expensive autocomplete seem like toddlers preordained to become gods or dictators. 

both quotes from Mandy Brown "Smoke Screen"; https://aworkinglibrary.com/writing/smoke-screen

> A postindustrial society must and can be so constructed that no one person’s ability to express him or herself in work will require as a condition the enforced labour or the enforced learning or the enforced consumption of another. ([View Highlight](https://read.readwise.io/read/01gsjzetpxr7ttvxrjb28esnss)) 

-- Ivan Illich writing "Conviviality" (1973): https://www.panarchy.org/illich/conviviality.html
- can there be an AI (narrow or general) that does not require the enforced consumption of another: even now if it is a case of the consumption of resources (most by industralised Global North)

> As feminist philosopher of science and technology, Donna Haraway, puts it: ‘objectivity turns out to be about particular and specific embodiment and definitely not about the false vision promising transcendence of all limits and responsibility’ (Haraway, 1988). According to Haraway and others, how we define our ideas about objectivity extend to the notion of what an object is in the first place and to the blurring of boundaries between self, technology and the world. As she pointed out when talking about the concept of situated knowledge, ‘objectivity cannot be about fixed vision when what counts as an object is precisely what world history turns out to be about’ (Haraway, 1988). [@mcquillan-2022-resisting Location 1852]






> Where does all this data come from? The answer is, of course, you—and probably everyone you know. Modern computer-vision applications are possible only because of the billions of images that internet users have uploaded and (sometimes) tagged with text identifying what is in the image.  [@mitchell-2019-artificial Location 1591]


- Note: Suchman. Weird how it only started to matter to artists when they had their work used for training sets. But not when all our family snaps with us and our bodies were being used without clear permission. Artists are special (again) -- seems like that would be about economics, and perhaps how concept of originality so tightly connected with being an artist??

No matter the question, the answer is "the body" (or variations on that). I share this. 


What of empiricism? That of direct experience through senses? Perhaps an old question given technologies have long awaited our capacity to (principally) see. Eg microscopes. These probabilistic methods imply superiority to "direct experience" (mcquillan). 


> The body is everywhere assaulted by all of our new media, a state which has resulted in deep disorientation of intellect and destabilization of culture throughout the world. In the age of disembodied communication, the meaning and significance and experience of the body is utterly transformed and distorted.

- Eric McCluhan (in https://www.chronogram.com/horoscopes/uranus-eris-and-the-riddle-of-the-internet-2373025)
- yes, son of Marshall. 

Sacasas:
At the moment, however, it appears that digital media tends toward political and epistemic fragmentation, not consensus, and toward the implausibility of any substantive account of the common good. (View Highlight) <https://read.readwise.io/read/01gvr33kd6qexa9xeb8zz6mjym>


> The self illusion distorts our own sense of agency, and distorts our attribution of agency to others. It leads us to see ourselves and others as free actors instead of embedded agents, leading to reactive attitudes that fail to reflect reality, and that do nobody any good. To see ourselves as interacting persons allows us to consider the causes and reasons for our own behavior and attitudes, as well as those of others, and encourages us to resolve problems rather than to recriminate, to ameliorate situations rather than to punish, and to cultivate attitudes that make everyone more effective and happy. ([View Highlight](https://read.readwise.io/read/01gttxknqy79z9n3zbrsc953fk)) - Garfield

+++++ 
- Note: Suchman: embedded agents. Things being done to us.


> Of course, any human-style explanation is not necessarily correct; we are fallible. But this is part of what it means to think: To be right, it must be possible to be wrong. ([View Highlight](https://read.readwise.io/read/01gw2fbn0ednkbtp04z3hetp2y))
- Chomsky (in Gruber NY Times post

> TED CHIANG: I tend to think that most fears about A.I. are best understood as fears about capitalism. And I think that this is actually true of most fears of technology, too. Most of our fears or anxieties about technology are best understood as fears or anxiety about how capitalism will use technology against us. And technology and capitalism have been so closely intertwined that it’s hard to distinguish the two. ([View Highlight](https://read.readwise.io/read/01gw6rm47h01jzecx28wmb7x4p))

> Śāntideva makes this point eloquently in *How to Lead an Awakened Life*. He argues that we can only cultivate attitudes of friendliness and care for others when we are able to project ourselves imaginatively into their situations, when we can regard others as our mothers, when we recognize our thoroughgoing interdependence with them, and when we abandon the fantasy that we and they are the agents of independent, free action. ([View Highlight](https://read.readwise.io/read/01gttx06s2d8x8r8z5v14rk6gj)) - Garfield
+++++ 
- Note: Suchman 

- Sacasas also mentions George Steiner's 1963 essay -- and how the untranslatability of advanced mathematics irrevocably changes the nature of politics (through language) -- when massive areas of understanding are subsumed by mathematics. Steiner viewed it as an attack on language in multiple domains. That language/speech is shrinking.  https://theconvivialsociety.substack.com/p/language-under-digital-conditions 


- what does 'direct experience' mean in the age of AI? -- what Sacasas calls "the algorithmic society"
- that these systems are beyond common understanding
- Arendt: speech bound to natality -- language that produces unpredictable consequences. 

- if it's not clear to people here I am not an expert with respect to AI and human technologies. I'm not even really a Monday morning quarterback. My expertise is the felt experiences and expressive potential of the body in motion. My assumption is that most of the people in this conversation today bend the same way (with exceptions to prove the rule).

> At Laguna Honda, lower-tech but human-paced, Dr. Sweet had the chance to practice a kind of “slow medicine” that has almost vanished. Gradually, the place and its patients transformed the way she understood the body. Alongside the modern view of the body as a machine to be fixed, her patients evoked an older notion, of the body as a garden to be tended. _God’s Hotel_tells their stories, and the story of the hospital, which — as efficiency experts, politicians, and architects descended, determined to turn it into a modern “health care facility” — revealed its truths about the cost and value of caring for body and soul.
- Victoria Sweet <https://www.victoriasweet.com/books/gods-hotel/>

> We have learned how to learn: much of our technology is devoted to devices that ensure the fidelity of our storage and transmission of information. We are born with an overwhelming inheritance. -- How We Are (Vincent Dreary)
- thought here is about fidelity. 

> I’m talking about machine learning. Which is itself one kind of story—one in which machines do something like “learn,” but which really means to memorize or put into storage, and includes nothing so pedestrian as understanding or interpreting. But the more common parlance—“artificial intelligence”—expands on that story to suggest that not only are the machines learning, but they have acquired the ability to think, or to intellectualize, implying that they have desires and personalities and behaviors. One way this story works is that by ascribing “thinking” to the machines, it triggers associations many people have with “higher” beings—whether species that are smarter than others, or people that are. (I’ll come back to this hierarchical notion of intelligence in a moment.)
- Mandy Brown <https://aworkinglibrary.com/writing/smoke-screen>

> All of this can be summed up briefly by observing that the human-built world is not built for humans. As the 20th-century French thinker Jacques Ellul noted, the operating principle of modern human society is technique, a relentless drive to optimize all things for efficiency. At no point is any care taken for the human as such. Even our games, diversions, and therapies can be best understood as what Ellul called human techniques, the bare minimum therapies necessary to keep the human component of the machine operational. https://theconvivialsociety.substack.com/p/what-you-get-is-the-world





> > This common, even instinctive, tendency is the moral counterpart and consequence of the view that we are selves. This is because it re-reinscribes the subject-object distinction that originates as an account of our *cognitive* relation to the world, but this time as an account of our *ethical* relation to the world. To see everything through the subject-object lens is to see oneself as subject and agent, and everyone else as moral objects and as patients. ([View Highlight](https://read.readwise.io/read/01gsxh3kc9jx7acfez9x0699pb)) -- Jay Garfield
- thinking here is about reseach is predicated on subject-object distinction

What does AI do? It reduces, fixes or solidifies that which is in movement.  

Dance talking into our constant change. We are not and nor can we ever be the same as before. How might this change reflect or contradict the absurdity of fixed identities. . 

Body as more than a metaphor (Damasio p.145). Tangibly analogue. Luddite. The slowness of development of perception. 

- change and mutability -- distinct from concepts. Hand is concept and doesn't change. 
- Body being / body known / body being known (distinct from I am a body, I have a body or I am knowing my body). Known by what? (Nothing to find). This is embodiment?
- Retreat Q&A -- from the Waking Up app. Click on the link to listen now. https://dynamic.wakingup.com/course/CA7D3D (about 35mins in)

About being in flux. 
What Are We? 
https://dynamic.wakingup.com/course/CB0627

> It was a reminder of what Rebecca Solnit repeats several times in A Paradise Built in Hell: The Extraordinary Communities that Arise in Disaster: “Beliefs matter.” (Location 391) - cited in Odell's Saving Time

+++++ 
- Note: Suchman. True stories are stories that are believed in



> She continues:
>  > Both types of experiences, second-person involvements and third-person observations, must influence each other and both may be necessary even for stable pre-inferential perceptions of other minds.… But being addressed as a You and addressing the other as a You arouses emotional responses differently from watching someone else being addressed, and engenders—even if briefly—a mutuality and suspension of separateness. The other becomes a person to you, someone who knocks you off balance or enters your consciousness in a more fundamental way than when you are largely untouched by the other, or just watching them.[17](https://readwise.io/reader/document_raw_content/32011253#chapter8-17) ([View Highlight](https://read.readwise.io/read/01gtr952qdxpgpc2gb2j96hddj))

- Note: Suchman: your body. Arouses different stuff from those watching. Self-evident perhaps. This is from Garfield

- also failing the octopus test: <https://read.readwise.io/read/01gtj1nyr7z4djxpb4cy0c0ga0>

> What this means, I think, is that AI as we know it today is designed to shift risks from systems to individuals, from the collective to the isolated. McQuillan notes this move in the various platforms (e.g. Uber moves risk from the company to its drivers; while Google shifts the burden of knowing what’s real from itself to its users), but once you see it there, it becomes impossible not to notice everywhere: in healthcare systems that determine which people deserve care, in flood maps that determine which neighborhoods get to rebuild, in generated lists of which jobs will be shuttered.  https://aworkinglibrary.com/writing/an-engine-of-precaritization

> “AI presents a technological shift in the framework of society that will amplify austerity while enabling authoritarian politics,” says Dan McQuillan on the first page of this book. He goes on to make the case that AI’s main moves are to segregate and divide, to make predictions of the future based on an extension of the past—that is, to preserve, and to increase, a status quo of inequality. It’s not so much that AI is fascist, he explains, so much that it is highly suitable to fascist aims, that it is a useful tool for those who desire to bring fascism about. But there’s hope, in that we can both learn to be attuned to when and how AI is recruited into authoritarian politics, and we can counter that recruitment through commoning, mutual aid, and solidarity.  https://aworkinglibrary.com/reading/resisting-ai
---

_original source/found:_ 

_reference:_ 



