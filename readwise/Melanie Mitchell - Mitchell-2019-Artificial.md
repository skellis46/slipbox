# Mitchell-2019-Artificial

![rw-book-cover](https://readwise-assets.s3.amazonaws.com/static/images/default-book-icon-4.11327a2af05a.png)

## Metadata
- Author: [[Melanie Mitchell]]
- Full Title: Mitchell-2019-Artificial
- Category: #books

## Highlights

> In short, Google is no longer merely a web-search portal—not by a long shot. It is rapidly becoming an applied AI company. AI is the glue that unifies the diverse products, services, and blue-sky research efforts offered by Google and its parent company, Alphabet. The company’s ultimate aspiration is reflected in the original mission statement of its DeepMind group: “Solve intelligence and use it to solve everything else.” (Location 44)


> As GEB made abundantly clear, Hofstadter firmly believes that the mind and all its characteristics emerge wholly from the physical substrate of the brain and the rest of the body, along with the body’s interaction with the physical world. There is nothing immaterial or incorporeal lurking there. The issue that worries him is really one of complexity. He fears that AI might show us that the human qualities we most value are disappointingly simple to mechanize. (Location 179)


> 1 The term artificial intelligence was McCarthy’s invention; he wanted to distinguish this field from a related effort called cybernetics.2 McCarthy later admitted that no one really liked the name—after all, the goal was genuine, not “artificial,” intelligence—but “I had to call it something, so I called it ‘Artificial Intelligence.’”3 (Location 257)


> “Define your terms … or we shall never understand one another.”10 This admonition from the eighteenth-century philosopher Voltaire is a challenge for anyone talking about artificial intelligence, because its central notion—intelligence—remains so ill-defined. Marvin Minsky himself coined the phrase “suitcase word”11 for terms like intelligence and its many cousins, such as thinking, cognition, consciousness, and emotion. Each is packed like a suitcase with a jumble of different meanings. Artificial intelligence inherits this packing problem, sporting different meanings in different contexts. (Location 286)


> In a recent report on the current state of AI, a committee of prominent researchers defined the field as “a branch of computer science that studies the properties of intelligence by synthesizing intelligence.”12 (Location 303)


> But how, exactly, can we determine the correct weights and threshold for a given task? Again, Rosenblatt proposed a brain-inspired answer: the perceptron should learn these values on its own. And how is it supposed to learn the correct values? Like the behavioral psychology theories popular at the time, Rosenblatt’s idea was that perceptrons should learn via conditioning. Inspired in part by the behaviorist psychologist B. F. Skinner, who trained rats and pigeons to perform tasks by giving them positive and negative reinforcement, Rosenblatt’s idea was that the perceptron should similarly be trained on examples: it should be rewarded when it fires correctly and punished when it errs. This form of conditioning is now known in AI as supervised learning. During training, the learning system is given an example, it produces an output, and it is then given a “supervision signal,” which tells how much the system’s output differs from the correct output. The system then uses this signal to adjust its weights and threshold. (Location 423)


> However, the human brain has given rise to language, which allows you to use symbols (words and phrases) to tell me—often imperfectly—what your thoughts are about or why you did a certain thing. In this sense, our neural firings can be considered subsymbolic, in that they underlie the symbols our brains somehow create. Perceptrons, as well as more complicated networks of simulated neurons, have been dubbed “subsymbolic” in analogy to the brain. (Location 479)


> Marvin Minsky pointed out that in fact AI research had uncovered a paradox: “Easy things are hard.” The original goals of AI—computers that could converse with us in natural language, describe what they saw through their camera eyes, learn new concepts after seeing only a few examples—are things that young children can easily do, but, surprisingly, these “easy things” have turned out to be harder for AI to achieve than diagnosing complex diseases, beating human champions at chess and Go, and solving complex algebraic problems. As Minsky went on, “In general, we’re least aware of what our minds do best.”27 The attempt to create artificial intelligence has, at the very least, helped elucidate how complex and subtle are our own minds. (Location 536)


> What we now call neural networks were then generally referred to as connectionist networks, where the term connectionist refers to the idea that knowledge in these networks resides in weighted connections between units. (Location 610)


> Indeed, by the mid-1980s, expert systems—symbolic AI approaches that rely on humans to create rules that reflect expert knowledge of a particular domain—were increasingly revealing themselves to be brittle: that is, error-prone and often unable to generalize or adapt when presented with new situations. In analyzing the limitations of these systems, researchers were discovering how much the human experts writing the rules actually rely on subconscious knowledge—what you might call common sense—in order to act intelligently. This kind of common sense could not easily be captured in programmed rules or logical deduction, and the lack of it severely limited any broad application of symbolic AI methods. In short, after a cycle of grand promises, immense funding, and media hype, symbolic AI was facing yet another AI winter. (Location 619)


> Subsymbolic systems seem much better suited to perceptual or motor tasks for which humans can’t easily define rules. You can’t easily write down rules for identifying handwritten digits, catching a baseball, or recognizing your mother’s voice; you just seem to do it automatically, without conscious thought. As the philosopher Andy Clark put it, the nature of subsymbolic systems is to be “bad at logic, good at Frisbee.”7 (Location 641)


> Machine-learning researchers disparagingly referred to symbolic AI methods as good old-fashioned AI, or GOFAI (pronounced “go-fye”),8 and roundly rejected them. (Location 653)


> The Google researchers didn’t tell the system to learn about any particular objects, but after a week of training, when they probed the innards of the network, what did they find? A “neuron” (unit) that seemed to encode cats.1 This self-taught cat-recognition machine was one of a series of impressive AI feats that have captured the public’s attention over the last decade. Most of these achievements rely on a set of neural network algorithms known as deep learning. (Location 666)


> Our collective human angst over Deep Blue quickly receded. We accepted that chess could yield to brute-force machinery; playing chess well, we allowed, didn’t require general intelligence after all. This seems to be a common response when computers surpass humans on a particular task; we conclude that the task doesn’t actually require intelligence. As John McCarthy lamented, “As soon as it works, no one calls it AI anymore.”4 (Location 680)


> As the technology journalist Kevin Kelly observed, “The business plans of the next 10,000 startups are easy to forecast: Take X and add AI.”5 And, crucially, for nearly all of these companies, AI has meant “deep learning.” (Location 699)


> A recent appraisal of the field stated this well: “A pile of narrow intelligences will never add up to a general intelligence. General intelligence isn’t about the number of abilities, but about the integration between those abilities.”9 (Location 721)


> Not surprisingly, in the AI research community there is considerable controversy over what human-level AI would entail. How can we know if we have succeeded in building such a “thinking machine”? Would such a system be required to have consciousness or self-awareness in the way humans do? Would it need to understand things in the same way a human understands them? Given that we’re talking about a machine here, would we be more correct to say it is “simulating thought,” or could we say it is truly thinking? (Location 729)


> From the vantage of many decades, my own vote for the strongest of Turing’s possible arguments is the “argument from consciousness,” which he summarizes by quoting the neurologist Geoffrey Jefferson: Not until a machine can write a sonnet or compose a concerto because of thoughts and emotions felt, and not by the chance fall of symbols, could we agree that machine equals brain—that is, not only write it but know that it had written it. No mechanism could feel (and not merely artificially signal, an easy contrivance) pleasure at its successes, grief when its valves fuse, be warmed by flattery, be made miserable by its mistakes, be charmed by sex, be angry or depressed when it cannot get what it wants.11 (Location 741)


> It’s just that she doesn’t think machines could ever have the right stuff to “actually think.” In (Location 767)


> In the academic realm, the most famous version of this argument was put forth by the philosopher John Searle. In 1980, Searle published an article called “Minds, Brains, and Programs”12 in which he vigorously argued against the possibility of machines actually thinking. In this widely read, controversial piece, Searle introduced the concepts of “strong” and “weak” AI in order to distinguish between two philosophical claims made about AI programs. While many people today use the phrase strong AI to mean “AI that can perform most tasks as well as a human” and weak AI to mean the kind of narrow AI that currently exists, Searle meant something different by these terms. For Searle, the strong AI claim would be that “the appropriately programmed digital computer does not just simulate having a mind; it literally has a mind.”13 In contrast, in Searle’s terminology, weak AI views computers as tools to simulate human intelligence and does not make any claims about them “literally” having a mind.14 (Location 768)


> Turing suggested the following: “The question, ‘Can machines think?’ should be replaced by ‘Are there imaginable digital computers which would do well in the imitation game?’” In other words, if a computer is sufficiently humanlike to be indistinguishable from humans, aside from its physical appearance or what it sounds like (or smells or feels like, for that matter), why shouldn’t we consider it to actually think? Why should we require an entity to be created out of a particular kind of material (for example, biological cells) to grant it “thinking” status? As the computer scientist Scott Aaronson put it bluntly, Turing’s proposal is “a plea against meat chauvinism.”16 (Location 787)


> These bots are, of course, leveraging our very human tendency to anthropomorphize (you were right, Mom!). We are all too willing to ascribe understanding and consciousness to computers, based on little evidence. (Location 833)


> Yet Kurzweil is best known not for his inventions but for his futurist prognostications, most notably the idea of the Singularity: “a future period during which the pace of technological change will be so rapid, its impact so deep, that human life will be irreversibly transformed.”19 Kurzweil uses the term singularity in the sense of “a unique event with … singular implications”; in particular, “an event capable of rupturing the fabric of human history.”20 For Kurzweil, this singular event is the point in time when AI exceeds human intelligence. (Location 844)


> The writer Andrian Kreye wryly referred to Kurzweil’s Singularity prediction as “nothing more than the belief in a technological Rapture.” (Location 878)


> But even if a machine operating on the brain’s principles can be created, how will it learn all the stuff it needs to know to be considered intelligent? After all, a newborn baby has a brain, but it doesn’t yet have what we’d call human-level intelligence. Kurzweil agrees: “Most of [the brain’s] complexity comes from its own interaction with a complex world. Thus, it will be necessary to provide an artificial intelligence with an education just as we do with a natural intelligence.” (Location 931)


> Mainstream AI’s attitude is perfectly captured in an article by the journalist Maureen Dowd: she describes how Andrew Ng, a famous AI researcher from Stanford, rolled his eyes at her mention of Kurzweil, saying, “Whenever I read Kurzweil’s Singularity, my eyes just naturally do that.”33 (Location 950)


> Mitchell Kapor. (Location 988)


> Kapor doesn’t buy it. His main argument centers on the influence of our (human) physical bodies and emotions on our cognition. “Perception of and [physical] interaction with the environment is the equal partner of cognition in shaping experience.… [Emotions] bound and shape the envelope of what is thinkable.”46 Kapor asserts that without the equivalent of a human body, and everything that goes along with it, a machine will never be able to learn all that’s needed to pass his and Kurzweil’s strict Turing test. (Location 1020)


> I assert that the fundamental mode of learning of human beings is experiential. Book learning is a layer on top of that.… If human knowledge, especially knowledge about experience, is largely tacit, i.e., never directly and explicitly expressed, it will not be found in books, and the Kurzweil approach to knowledge acquisition will fail.… It is not in what the computer knows but what the computer does not know and cannot know wherein the problem resides.47 (Location 1025)


+++++ 
- Note: Suchman


> we need to take a careful look at some of the crucial abilities underlying our distinctive human intelligence, such as perception, language, decision-making, commonsense reasoning, and learning. (Location 1048)


+++++ 
- Note: Suchman


> When you look at this picture, at the most basic level you’re seeing bits of ink on a page (or pixels on a screen). Somehow your eyes and brain are able to take in this raw information and, within a few seconds, transform it into a detailed story involving living things, objects, relationships, places, emotions, motivations, and past and future actions. We look, we see, we understand. Crucially, we know what to ignore. There are many aspects of the photo that aren’t strictly relevant to the story we extract from it: the pattern on the carpet, the hanging straps on the soldier’s backpack, the whistle clipped to her pack’s shoulder pad, the barrettes in her hair. (Location 1069)


> Vision—both looking and seeing—turns out to be one of the hardest of all “easy” things. (Location 1087)


+++++ 
- Note: And what of Sensing body as direct experience?


> Nonetheless, the neocognitron was an important inspiration for later approaches to deep neural networks, including today’s most influential and widely used approach: convolutional neural networks, or (as most people in the field call them) ConvNets. ConvNets are the driving force behind today’s deep-learning revolution in computer vision, and in other areas as well. Although they have been widely heralded as the next big thing in AI, ConvNets are actually not very new: they were first proposed in the 1980s by the French computer scientist Yann LeCun, who had been inspired by Fukushima’s neocognitron. (Location 1124)


> Following this procedure—input the image, then calculate the error at the output, then change the weights—for every image in your training set is called one “epoch” of training. Training a ConvNet requires many epochs, during which the network processes each image over and over again. Initially, the network will be very bad at recognizing dogs and cats, but slowly, as it changes its weights over many epochs, it will get increasingly better at the task. Finally, at some point, the network “converges”; that is, the weights stop changing much from one epoch to the next, and the network is (in principle!) very good at recognizing dogs and cats in the images in the training set. (Location 1269)


> Yann LeCun himself was taken by surprise at how fast things turned around for his ConvNets: “It’s rarely the case where a technology that has been around for 20, 25 years—basically unchanged—turns out to be the best. The speed at which people have embraced it is nothing short of amazing. I’ve never seen anything like this before.” (Location 1451)


> When you read about a machine “identifying objects correctly,” you’d think that, say, given an image of a basketball, the machine would output “basketball.” But of course, on ImageNet, correct identification means only that the correct category is in the machine’s top-five categories. If, given an image of a basketball, the machine outputs “croquet ball,” “bikini,” “warthog,” “basketball,” and “moving van,” in that order, it is considered correct. (Location 1486)


> The localization task provided training images with such boxes drawn (by Mechanical Turk workers) around the target object(s) in each image; on the test images, the task for competing programs was to predict five object categories each with the coordinates of a corresponding box. (Location 1515)


+++++ 
- Note: Mechanical Turk artwoek. Fascinating


> As the computer-vision expert Ali Farhadi told The New York Times, “We’re still very, very far from visual intelligence, understanding scenes and actions the way humans do.”18 Why are we still so far from this goal? It seems that visual intelligence isn’t easily separable from the rest of intelligence, especially general knowledge, abstraction, and language—abilities that, interestingly, involve parts of the brain that have many feedback connections to the visual cortex. (Location 1533)


> The learning-from-data approach of deep neural networks has generally proved to be more successful than the “good old-fashioned AI” strategy, in which human programmers construct explicit rules for intelligent behavior. However, contrary to what some media have reported, the learning process of ConvNets is not very humanlike. (Location 1558)


> In contrast, even the youngest children learn an open-ended set of categories and can recognize instances of most categories after seeing only a few examples. Moreover, children don’t learn passively: they ask questions, they demand information on the things they are curious about, they infer abstractions of and connections between concepts, and, above all, they actively explore the world. (Location 1562)


> While ConvNets use back-propagation to learn their “parameters” (that is, weights) from training examples, this learning is enabled by a collection of what are called “hyperparameters”—an umbrella term that refers to all the aspects of the network that need to be set up by humans to allow learning to even begin. (Location 1568)


> Tuning the hyperparameters might sound like a pretty mundane activity, but doing it well is absolutely crucial to the success of ConvNets and other machine-learning systems. Because of the open-ended nature of designing these networks, in general it is not possible to automatically set all the parameters and designs, even with automated search. Often it takes a kind of cabalistic knowledge that students of machine learning gain both from their apprenticeships with experts and from hard-won experience. As Eric Horvitz, director of Microsoft’s research lab, characterized it, “Right now, what we are doing is not a science but a kind of alchemy.”5 And the people who can do this kind of “network whispering” form a small, exclusive club: according to Demis Hassabis, cofounder of Google DeepMind, “It’s almost like an art form to get the best out of these systems.… There’s only a few hundred people in the world that can do that really well.”6 (Location 1575)


> Where does all this data come from? The answer is, of course, you—and probably everyone you know. Modern computer-vision applications are possible only because of the billions of images that internet users have uploaded and (sometimes) tagged with text identifying what is in the image. (Location 1591)


+++++ 
- Note: Suchman. Weitd how it only started to matter when artists had their work used for training sets. But not when all our famipy snaps with us and our bodies.


> Self-driving car companies collect these training examples from countless hours of video taken by cameras mounted on actual cars driving in traffic on highways and city streets. These cars may be self-driving prototypes being tested by companies or, in the case of Tesla, cars driven by customers who, upon purchase of a Tesla vehicle, must agree to a data-sharing policy with the company.9 (Location 1614)


> In 2017, the Financial Times reported that “most companies working on this technology employ hundreds or even thousands of people, often in offshore outsourcing centres in India or China, whose job it is to teach the robo-cars to recognize pedestrians, cyclists and other obstacles. The workers do this by manually marking up or ‘labeling’ thousands of hours of video footage, often frame by frame.”10 New companies have sprung up to offer labeling data as a service; Mighty AI, for example, offers “the labeled data you need to train your computer vision models” and promises “known, verified, and trusted annotators who specialize in autonomous driving data.”11 (Location 1619)


+++++ 
- Note: Suchman. The bodies of thoselabelling training sets


> The workers do this by manually marking up or ‘labeling’ thousands of hours of video footage, often frame by frame.”10 New companies have sprung up to offer labeling data as a service; Mighty AI, for example, offers “the labeled data you need to train your computer vision models” and promises “known, verified, and trusted annotators who specialize in autonomous driving data.”11 The Long Tail The supervised-learning approach, using large data sets and armies of human annotators, works well for at least some of the visual abilities needed for self-driving cars (many companies are also exploring the use of video-game-like driving-simulation programs to augment supervised training). (Location 1621)


> As the renowned AI researcher Andrew Ng has warned, “Requiring so much data is a major limitation of [deep learning] today.”12 Yoshua Bengio, another high-profile AI researcher, agrees: “We can’t realistically label everything in the world and meticulously explain every last detail to the computer.”13 (Location 1630)


> Yann LeCun himself acknowledges that “unsupervised learning is the dark matter of AI.” In other words, for general AI, almost all learning will have to be unsupervised, but no one has yet come up with the kinds of algorithms needed to perform successful unsupervised learning. (Location 1668)


> But humans also have a fundamental competence lacking in all current AI systems: common sense. (Location 1671)


> Many people believe that until AI systems have common sense as humans do, we won’t be able to trust them to be fully autonomous in complex real-world situations. (Location 1677)


> The network was trained on photos like the ones in figure 15, and it performed very well on this task on the test set. But what did the network actually learn? By performing a careful study, Will found an unexpected answer: in part, the network learned to classify images with blurry backgrounds as “contains an animal,” whether or not the image actually contained an animal.14 The nature photos in the training and test sets obeyed an important rule of photography: focus on the subject of the photo. (Location 1681)


> This is an example of a common phenomenon seen in machine learning. The machine learns what it observes in the data rather than what you (the human) might observe. (Location 1691)


> In machine-learning jargon, Will’s network “overfitted” to its specific training set, and thus can’t do a good job of applying what it learned to images that differ from those it was trained on. (Location 1695)


> One group showed that if ConvNets are trained on images downloaded from the web (like those in ImageNet), they perform poorly on images that were taken by a robot moving around a house with a camera.15 It seems that random views of household objects can look very different from photos that people put on the web. (Location 1698)


+++++ 
- Note: Amazing. For art project. Who is taking thr photo matters (Shaun project)


> Other groups have shown that superficial changes to images, such as slightly blurring or speckling an image, changing some colors, or rotating objects in the scene, can cause ConvNets to make significant errors even when these perturbations don’t affect humans’ recognition of objects.16 This unexpected fragility of ConvNets—even those that have been said to “surpass humans at object recognition”—indicates that they are overfitting to their training data and learning something different from what we are trying to teach them. (Location 1701)


> Kate Crawford, a researcher at Microsoft and an activist for fairness and transparency in AI, pointed out that one widely used data set for training face-recognition systems contains faces that are 77.5 percent male and 83.5 percent white. (Location 1721)


> Should the data sets being used to train AI accurately mirror our own biased society—as they often do now—or should they be tinkered with specifically to achieve social reform aims? And who should be allowed to specify the aims or do the tinkering? (Location 1736)


> A list of a billion operations is not an explanation that a human can understand. Even the humans who train deep networks generally cannot look under the hood and provide explanations for the decisions their networks make. MIT’s Technology Review magazine called this impenetrability “the dark secret at the heart of AI.”19 The fear is that if we don’t understand how AI systems work, we can’t really trust them or predict the circumstances under which they will make errors. (Location 1749)


> It shouldn’t come as a surprise then that one of the hottest new areas of AI is variously called “explainable AI,” “transparent AI,” or “interpretable machine learning.” These terms refer to research on getting AI systems—particularly deep networks—to explain their decisions in a way that humans can understand. (Location 1760)


> However, a year after AlexNet’s win, a research paper appeared, authored by Christian Szegedy of Google and several others, with the deceptively mild title “Intriguing Properties of Neural Networks.”20 One of the “intriguing properties” described in the paper was that AlexNet could easily be fooled. (Location 1774)


> In particular, the paper’s authors had discovered that they could take an ImageNet photo that AlexNet classified correctly with high confidence (for example, “School Bus”) and distort it by making very small, specific changes to its pixels so that the distorted image looked completely unchanged to humans but was now classified with very high confidence by AlexNet as something completely different (for example, “Ostrich”). The authors called the distorted image an “adversarial example.” Figure 18 shows a few samples of original images and their adversarial twins. Can’t tell the difference? Congratulations! It seems that you are human. (Location 1777)


> Calling this an “intriguing property” of neural networks is a little like calling a hole in the hull of a fancy cruise liner a “thought-provoking facet” of the ship. Intriguing, yes, and more investigation is needed, but if the leak is not fixed, this ship is going down. (Location 1791)


> a group from the University of Wyoming published an article with a more direct title: “Deep Neural Networks Are Easily Fooled.”21 By using a biologically inspired computational method called genetic algorithms,22 the Wyoming group was able to computationally “evolve” images that look like random noise to humans but for which AlexNet and other convolutional neural networks assigned specific object categories with greater than 99 percent confidence. Figure 19 shows some examples. The Wyoming group noted that deep neural networks (DNNs) “see these objects as near-perfect examples of recognizable images,” which “[raises] questions about the true generalization capabilities of DNNs and the potential for costly exploits [that is, malicious applications] of solutions that use DNNs.”23 (Location 1793)


> FIGURE 19: Examples of images created by a genetic algorithm specifically to fool a convolutional neural network. In each case, AlexNet (trained on the ImageNet training set) assigned a confidence greater than 99 percent that the image was an instance of the category shown. (Location 1802)


> All this has reenergized the small research community focusing on “adversarial learning”—that is, developing strategies that defend against potential (human) adversaries who could attack machine-learning systems. Adversarial-learning researchers often start their work by demonstrating possible ways in which existing systems can be attacked, and some of the recent demonstrations have been stunning. (Location 1810)


> FIGURE 20: An AI researcher (left) wearing eyeglass frames with a pattern specially designed to cause a deep neural network face recognizer, trained on celebrity faces, to confidently classify the left photo as the actress Milla Jovovich (right). The paper describing this study gives many other examples of impersonation using “adversarial” eyeglass-frame patterns. (Location 1823)


> Ian Goodfellow, an AI expert who is part of the Google Brain team, says, “Almost anything bad you can think of doing to a machine-learning model can be done right now … and defending it is really, really hard.”27 (Location 1835)


> To my mind, the ultimate problem is one of understanding. Consider figure 18, where AlexNet mistakes a school bus for an ostrich. Why would this be very unlikely to happen to a human? Even though AlexNet performs very well on ImageNet, we humans understand many things about the objects we see that are unknown to AlexNet or any other current AI system. We know what objects look like in three dimensions and can imagine this from a two-dimensional photo. We know what the function of a given object is, what role the object’s parts play in its overall function, and in what contexts an object usually appears. Seeing an object brings up memories of seeing such objects in other circumstances, from other viewpoints, as well as in other sensory modalities (we remember what a given object feels like, smells like, perhaps what it sounds like when dropped, and so on). All of this background knowledge feeds into the human ability to robustly recognize a given object. Even the most successful AI vision systems lack this kind of understanding and the robustness that it confers. (Location 1841)


> Unlike today’s ConvNets, human (and animal) perception is highly regulated by cognition—the kind of context-dependent understanding that I described above (Location 1856)


> Jeff Clune, an AI researcher at the University of Wyoming, made a very provocative analogy when he noted that there is “a lot of interest in whether Deep Learning is ‘real intelligence’ or a ‘Clever Hans.’”28 (Location 1862)


> Clever Hans has become a metaphor for any individual (or program!) that gives the appearance of understanding but is actually responding to unintentional cues given by a trainer. (Location 1868)


> Recognition of the success of these networks must be tempered with a realization that they can fail in unexpected ways because of overfitting to their training data, long-tail effects, and vulnerability to hacking. (Location 1876)


> Will the fact that these systems lack humanlike understanding inevitably render them fragile, unreliable, and vulnerable to attacks? And how should this factor into our decisions about applying AI systems in the real world? (Location 1880)


> Machine learning is being deployed to make decisions affecting the lives of humans in many domains. What assurances do you have that the machines creating your news feed, diagnosing your diseases, evaluating your loan applications, or—God forbid—recommending your prison sentence have learned enough to be trustworthy decision makers? (Location 1897)


> Current AI technology is central to services you yourself might use all the time, sometimes without even knowing that AI is involved, including speech transcription, GPS navigation and trip planning, email spam filters, language translation, credit-card fraud alerts, book and music recommendations, protection against computer viruses, and optimizing energy usage in buildings. (Location 1904)


> In the near future, AI applications will likely be widespread in health care. We will see AI systems assisting physicians in diagnosing diseases and in suggesting treatments; discovering new drugs; and monitoring the health and safety of the elderly in their homes. Scientific modeling and data analysis will increasingly rely on AI tools—for example, in improving models of climate change, population growth and demographic change, ecological and food science, and other major issues that society will be facing over the next century. For Demis Hassabis, the cofounder of Google’s DeepMind group, this is the most important potential benefit of AI: We might have to come to the sobering realisation that even with the smartest set of humans on the planet working on these problems, these [problems] may be so complex that it’s difficult for individual humans and scientific experts to have the time they need in their lifetimes to even innovate and advance.… It’s my belief we’re going to need some assistance and I think AI is the solution to that.1 (Location 1914)


> The AI researcher Andrew Ng has optimistically proclaimed, “AI is the new electricity.” Ng explains further: “Just as electricity transformed almost everything 100 years ago, today I actually have a hard time thinking of an industry that I don’t think AI will transform in the next several years.”2 This is an appealing analogy: the idea that soon AI will be as necessary—and as invisible—in our electronic devices as electricity itself. However, a major difference is that the science of electricity was well understood before it was widely commercialized. We are good at predicting the behavior of electricity. This is not the case for many of today’s AI systems. (Location 1937)


> Consider FaceFirst, a company that offers face-recognition services for a fee. As reported by the magazine New Scientist, “Face First … is rolling out a system for retailers that it says will ‘boost sales by recognizing high-value customers each time they shop’ and send ‘alerts when known litigious individuals enter any of your locations.’”6 Many other companies offer similar services. (Location 1978)


> For example, Brian Brackeen, the CEO of the face-recognition company Kairos, wrote the following in a widely circulated article: Facial recognition technologies, used in the identification of suspects, negatively affects people of color. To deny this fact would be a lie.… I (and my company) have come to believe that the use of commercial facial recognition in law enforcement or in government surveillance of any kind is wrong—and that it opens the door for gross misconduct by the morally corrupt.… We deserve a world where we’re not empowering governments to categorize, track and control citizens.8 (Location 1992)


> Given the risks of AI technologies, many practitioners of AI, myself included, are in favor of some kind of regulation. But the regulation shouldn’t be left solely in the hands of AI researchers and companies. The problems surrounding AI—trustworthiness, explainability, bias, vulnerability to attack, and morality of use—are social and political issues as much as they are technical ones. (Location 2011)


> I believe that regulation of AI should be modeled on the regulation of other technologies, particularly those in biological and medical sciences, such as genetic engineering. In those fields, regulation—such as quality assurance and the analysis of risks and benefits of technologies—occurs via cooperation among government agencies, companies, nonprofit organizations, and universities. Moreover, there are now established fields of bioethics and medical ethics, which have considerable influence on decisions about the development and application of technologies. AI research and its applications very much need a well-thought-out regulatory and ethics infrastructure. (Location 2023)


> One stumbling block is that there is no general agreement in the field on what the priorities for developing regulation and ethics should be. Should the immediate focus be on algorithms that can explain their reasoning? On data privacy? On robustness of AI systems to malicious attacks? On bias in AI systems? On the potential “existential risk” from superintelligent AI? My own opinion is that too much attention has been given to the risks from superintelligent AI and far too little to deep learning’s lack of reliability and transparency and its vulnerability to attacks. (Location 2036)


> In his commentary on this study, the psychologist Joshua Greene noted, “Before we can put our values into machines, we have to figure out how to make our values clear and consistent.”22 This seems to be harder than we might have thought. (Location 2093)


> To my mind, progress on giving computers moral intelligence cannot be separated from progress on other kinds of intelligence: the true challenge is to create machines that can actually understand the situations that they confront. (Location 2098)


> And as she wrote in The New York Times’ Modern Love column, “Eventually it hit me that the same techniques might work on that stubborn but lovable species, the American husband.” Sutherland wrote about how, after years of futile nagging, sarcasm, and resentment, she used this simple method to covertly train her oblivious husband to pick up his socks, find his own car keys, show up to restaurants on time, and shave more regularly.1 (Location 2118)


> Deciding how much to explore new actions and how much to exploit (that is, stick with) tried-and-true actions is called the exploration versus exploitation balance. Achieving the right balance is a core issue for making reinforcement learning successful. (Location 2257)


> In 2013, a group of Canadian AI researchers released a software platform called the Arcade Learning Environment that made it easy to test machine-learning systems on forty-nine of these games.3 This was the platform used by the DeepMind group in their work on reinforcement learning. (Location 2321)


> Richard Sutton, one of the originators of this method, calls this “learning a guess from a guess.”5 I’ll amend that to “learning a guess from a better guess.” (Location 2361)


> In short, instead of learning to match its outputs to human-given labels, the network learns to make its outputs consistent from one iteration to the next, assuming that later iterations give better estimates of value than earlier iterations. This learning method is called temporal difference learning. (Location 2363)


> The DeepMind group applied their deep Q-learning method to the forty-nine different Atari games in the Arcade Learning Environment. While DeepMind’s programmers used the same network architecture and hyperparameter settings for each game, their system learned each game from scratch; that is, the system’s knowledge (the network weights) learned for one game was not transferred when the system started learning to play the next game. Each game required training for thousands of episodes, but this could be done relatively quickly on the company’s advanced computer hardware. (Location 2376)


> As Claude Shannon wrote presciently in 1950, a machine that can surpass humans at chess “will force us either to admit the possibility of mechanized thinking or to further restrict our concept of thinking.”17 The latter happened. Superhuman chess playing is now seen as something that doesn’t require general intelligence. Deep Blue isn’t intelligent in any sense we mean today. It can’t do anything but play chess, and it doesn’t have any conception of what “playing a game” or “winning” means to humans. (I once heard a speaker say, “Deep Blue may have beat Kasparov, but it didn’t get any joy out of it.”) (Location 2482)


> With its AlphaGo project, DeepMind demonstrated that one of AI’s longtime grand challenges could be conquered by an inventive combination of reinforcement learning, convolutional neural networks, and Monte Carlo tree search (and adding powerful modern computing hardware to the mix). (Location 2621)


> Humans exhibit this kind of transfer from one task to another seemingly effortlessly; our ability to generalize what we learn is a core part of what it means for us to think. Thus, in human-speak, we might say that another term for transfer learning is, well, learning. (Location 2653)


> However, as Gary Marcus pointed out, there are many games humans play that are even more challenging for AI than Go. One striking example Marcus gives is charades,8 which, if you think about it, requires sophisticated visual, linguistic, and social understanding far beyond the abilities of any current AI system. If you could build a robot that could play charades as well as, say, a six-year-old child, then I think you could safely say that you had conquered several of the “most challenging of domains” for AI. (Location 2692)


> As Marcus notes, while we humans attribute to the program a certain understanding of what we consider basic concepts (for example, wall, ceiling, paddle, ball, tunneling), the program actually has no such concepts: These demonstrations make clear that it is misleading to credit deep reinforcement learning with inducing concepts like wall or paddle; rather, such remarks are what comparative (animal) psychology sometimes call overattributions. It’s not that the Atari system genuinely learned a concept of wall that was robust but rather the system superficially approximated breaking through walls within a narrow set of highly trained circumstances.12 (Location 2719)


> In short, while these deep Q-learning systems have achieved superhuman performance in some narrow domains, and even exhibit what resembles “intuition” in these domains, they are lacking something absolutely fundamental to human intelligence. Whether it is called abstraction, domain generalization, or transfer learning, imbuing systems with this ability is still one of AI’s most important open problems. (Location 2728)


> AlphaGo, in spite of the millions of games it has played during its training, has not learned to “think” better about anything except the game of Go. In fact, it has no ability to think about anything, to reason about anything, to make plans about anything, except Go. (Location 2742)


> AlphaGo is the ultimate idiot savant. (Location 2745)


> For humans, a crucial part of intelligence is, rather than being able to learn any particular skill, being able to learn to think and to then apply our thinking flexibly to whatever situations or challenges we encounter. (Location 2749)


> The real world doesn’t come so cleanly delineated. Douglas Hofstadter has pointed out that the very notion of a clearly defined “state” isn’t at all realistic. “If you look at situations in the world, they don’t come framed, like a chess game or a Go game.… A situation in the world is something that has no boundaries at all; you don’t know what’s in the situation, what’s out of the situation.”13 (Location 2762)


> All these issues led Andrej Karpathy, Tesla’s director of AI, to note that, for real-world tasks like this, “basically every single assumption that Go satisfies and that AlphaGo takes advantage of are violated, and any successful approach would look extremely different.”15 (Location 2782)


> This part of the book deals with natural-language processing, which means “getting computers to deal with human language.” (In AI-speak, “natural” means “human.”) Natural-language processing (abbreviated NLP) includes topics such as speech recognition, web search, automated question answering, and machine translation. (Location 2808)


> it seems to be impossible to capture the subtleties of language by applying a set of explicit rules. (Location 2844)

## New highlights added April 11, 2023 at 5:12 PM

> documents as vectors, though with mixed success. Reducing all of semantics to geometry is an alluring idea for AI researchers. “I think you can capture a thought by a vector,” proclaimed Google’s Geoffrey Hinton.21 Facebook’s Yann LeCun concurred: “[At Facebook AI Research] we want to embed the world in thought vectors. We call this World2Vec.”22 One last (Location 3107)


> Reducing all of semantics to geometry is an alluring idea for AI researchers. “I think you can capture a thought by a vector,” proclaimed Google’s Geoffrey Hinton.21 Facebook’s Yann LeCun concurred: “[At Facebook AI Research] we want to embed the world in thought vectors. We call this World2Vec.”22 (Location 3108)


> Facebook’s executive in charge of language translation told an audience, “What we believe is that neural networks are learning the underlying semantic meaning of the language.”14 The CEO of the specialty translation company DeepL bragged, “Our [machine-translation] neural networks have developed an astounding sense of understanding.”15 (Location 3220)


> for small amounts of text, if you’re a company (Location 3226)


> The main obstacle is this: like speech-recognition systems, machine-translation systems perform their task without actually understanding the text they are processing.21 In translation as well as in speech recognition, the question remains: To what extent is such “understanding” needed for machines to reach human levels of performance? Douglas Hofstadter argues, “Translation is far more complex than mere dictionary look-up and word rearranging.… Translation involves having a mental model of the world being discussed.”22 For example, a human translating the “Restaurant” story would have a mental model in which, when a man storms out of a restaurant without paying, a waitress would be more likely to shout at him about paying for his meal than about “proposed legislation.” Hofstadter’s words were echoed in a recent article by the AI researchers Ernest Davis and Gary Marcus: “Machine translation … often involves problems of ambiguity that can only be resolved by achieving an actual understanding of the text—and bringing real-world knowledge to bear.”23 (Location 3313)


> The images were downloaded from repositories such as Flickr.com, and the captions for these images were produced by humans—namely, Amazon Mechanical Turk workers, who were hired by Google for this study. Because captions can be so variable, each image was given a caption by five different people. Thus, each image appears in the training set five times, each time paired with a different caption. (Location 3347)


> However, these systems don’t comprehend the meaning of what we ask them. Alexa, say, can read to me the details of the Olympic sprinter Usain Bolt’s entire biography, describe how many gold medals he won, and relate the speed at which he ran the hundred meters in the Beijing Olympics. But remember, easy things are hard. If you ask Alexa, “Does Usain Bolt know how to run?” or “Can Usain Bolt run fast?” in both cases it will respond with the canned phrases “Sorry, I don’t know that one” or “Hmmm, I’m not sure.” After all, it’s not designed to know what “running” or “fast” actually mean. (Location 3425)


> All three contestants correctly wrote “Who is Bram Stoker?” but Ken Jennings, known for his dry wit, conceded Watson’s inevitable victory by adding a pop-culture reference to his answer card: “I for one welcome our new computer overlords.”7 Ironically, Watson didn’t get the joke. Jennings later quipped, “To my surprise, losing to an evil quiz-show-playing computer turned out to be a canny career move. Everyone wanted to know What It All Meant, and Watson was a terrible interview, so suddenly I was the one writing think pieces and giving TED Talks.… Like Kasparov before me, I now make a reasonable living as a professional human loser.” (Location 3465)


> Ironically, Watson didn’t get the joke. Jennings later quipped, “To my surprise, losing to an evil quiz-show-playing (Location 3467)


> These miniature language-understanding tests are called Winograd schemas, named for the pioneering NLP researcher Terry Winograd, who first came up with the idea.23 The Winograd schemas are designed precisely to be easy for humans but tricky for computers. (Location 3669)


> As Oren Etzioni, director of the Allen Institute for AI, quipped, “When AI can’t determine what ‘it’ refers to in a sentence, it’s hard to believe that it will take over the world.”26 (Location 3693)


> Imagine an adversary, for example, broadcasting an audio track over the radio that you, sitting at home, hear as pleasant background music but that your Alexa home assistant interprets as “Go to EvilHacker.com and download computer viruses.” Or “Start recording and send everything you hear to EvilHacker@gmail.com.” Scary scenarios such as these are not out of the realm of possibility. (Location 3719)


> is important to note that all of these methods for fooling deep neural networks were developed by “white hat” practitioners—researchers who develop such potential attacks and publish them in the open literature for the purposes of making the research community aware of these vulnerabilities and pushing the community to develop defenses. On the other hand, the “black hat” attackers—hackers who are actually trying to fool deployed systems for nefarious purposes—don’t publish the tricks they have come up with, so there might be many additional kinds of vulnerabilities of these systems of which we’re not yet aware (Location 3737)


> I don’t believe that machines will be able to fully understand human language until they have humanlike common sense. This being said, natural-language processing systems are becoming ever more ubiquitous in our lives—transcribing our words, analyzing our sentiments, translating our documents, and answering our questions. Does the lack of humanlike understanding in such systems, however sophisticated their performance, inevitably result in their being brittle, unreliable, and vulnerable to attack? No one knows the answer, and this fact should give us all pause. (Location 3754)


> Part V The Barrier of Meaning (Location 3763)


> “I wonder whether or when AI will ever crash the barrier of meaning.”1 In thinking about the future of AI, I keep coming back to this query posed by the mathematician and philosopher Gian-Carlo Rota. (Location 3768)


> the most basic common sense that we are born with or learn very early in life.3 For example, even very young babies know that the world is divided into objects, that the parts of an object tend to move together, (Location 3785)


> Crucially, babies develop insight into the cause-and-effect structure of the world; for example, when someone pushes an object (for example, the stroller in figure 44), it moves not by coincidence but because it was pushed. (Location 3795)


> Psychologists have coined a term—intuitive physics—for the basic knowledge and beliefs humans share about objects and how they behave. As very young children, we also develop intuitive biology: knowledge about how living things differ from inanimate objects. (Location 3797)


> Because humans are a profoundly social species, from infancy on we additionally develop intuitive psychology: the ability to sense and predict the feelings, beliefs, and goals of other people. (Location 3801)


> The psychologist Lawrence Barsalou is one of the best-known proponents of the “understanding as simulation” hypothesis. In his view, our understanding of the situations we encounter consists in our (subconsciously) performing these kinds of mental simulations. Moreover, Barsalou has proposed that such mental simulations likewise underlie our understanding of situations that we don’t directly participate in—that is, situations we might watch, hear, or read about. He writes, “As people comprehend a text, they construct simulations to represent its perceptual, motor, and affective content. Simulations appear central to the representation of meaning.”7 (Location 3830)


> According to Barsalou, “conceptual processing uses reenactments of sensory-motor states—simulations—to represent categories,”8 even the most abstract ones. Surprisingly (at least to me), some of the most compelling evidence for this hypothesis comes from the cognitive study of metaphor. (Location 3840)


> Lakoff and Johnson’s thesis is that not only is our everyday language absolutely teeming with metaphors that are often invisible to us, but our understanding of essentially all abstract concepts comes about via metaphors based on core physical knowledge. (Location 3855)


+++++ 
- Note: Suchman. Expansion of receptiveness to direct physical experience.


> what we refer to as perception, categorization, recognition, generalization, and reminding (“the exact same thing happened to me”) all involve the act of abstracting the situations that we experience. (Location 3936)


> Douglas Hofstadter, who has studied abstraction and analogy making for several decades, defines analogy making in a very general sense as “the perception of a common essence between two things.” (Location 3938)


> As Hofstadter and his coauthor, the psychologist Emmanuel Sander, stated, “Without concepts there can be no thought, and without analogies there can be no concepts.”17 (Location 3946)


> I certainly don’t claim to have covered all of the components of human understanding. Indeed, many people have noted that the terms understanding and meaning (not to mention consciousness) are merely ill-defined terms that we use as placeholders, because we don’t yet have the correct language or theory to talk about what’s actually going on in the brain. (Location 3953)


> Cyc is a symbolic AI system of the kind I described in chapter 1—a collection of statements (“assertions”) about specific entities or general concepts, written in a logic-based computer language. Here are some examples of Cyc’s assertions (translated from logical form into English):2 •  An entity cannot be in more than one place at the same time. •  Objects age one year per year. •  Each person has a mother who is a female person. (Location 3989)


> In today’s AI landscape dominated by deep learning, the Cyc project is one of the last remaining large-scale symbolic AI efforts.5 (Location 4009)


> However, humanlike abstraction and analogy making are not skills that can be captured by Cyc’s massive set of facts or, I believe, by logical inference in general. (Location 4017)


> I’ve seen something of a shift in the conversation lately: increasingly, the AI community is once again talking about the paramount importance of giving machines common sense. In 2018, Microsoft’s cofounder Paul Allen doubled the budget of the research institute he founded, the Allen Institute for AI, specifically to study common sense. (Location 4043)


> The [funding] program will create more human-like knowledge representations, for example, perceptually-grounded representations, to enable commonsense reasoning by machines about the physical world and spatio-temporal phenomena.”12 (Location 4048)


> As the cognitive scientist Robert French phrased it, abstraction and analogy are all about perceiving “the subtlety of sameness.”15 (Location 4075)


> In short, today’s ConvNets, while remarkably adept at learning the features needed to recognize ImageNet objects or to choose moves in Go, do not have what it takes to do the kinds of abstraction and analogy making required even in Bongard’s idealized problems, much less in the real world. (Location 4120)


> These alternative answers might seem overly literal-minded, but there’s no strictly logical argument that says they are wrong. In fact, there are infinitely many possible rules one might infer. Why do most people agree that one of them (pqrt) is the best? It seems that our mental mechanisms for abstraction—which evolved to promote our survival and reproduction in the real world—carry over to this idealized microworld. (Location 4142)


> However, while analogy is fundamental to human cognition at every level, there are as yet no AI programs that come remotely close to human analogy-making abilities. (Location 4232)


> The modern age of artificial intelligence is dominated by deep learning, with its triumvirate of deep neural networks, big data, and ultrafast computers. However, in the quest for robust and general intelligence, deep learning may be hitting a wall: the all-important “barrier of meaning.” (Location 4235)


> Karpathy notes: “You are reasoning about [the] state of mind of people, and their view of the state of mind of another person. That’s getting frighteningly meta.” In summary, “It is mind-boggling that all of the above inferences unfold from a brief glance at a 2D array of [pixel] values.” (Location 4264)


> Karpathy concludes his post with this thought: A seemingly inescapable conclusion for me is that we may … need embodiment, and that the only way to build computers that can interpret scenes like we do is to allow them to get exposed to all the years of (structured, temporally coherent) experience we have, ability to interact with the world, and some magical active learning/inference architecture that I can barely even imagine when I think backwards about what it should be capable of. (Location 4269)


+++++ 
- Note: Suchman


> Since the 1950s, the dominant approaches to AI have implicitly embraced Descartes’s thesis, assuming that general intelligence can be attained by disembodied computers. However, a small segment of the AI community has consistently argued for the so-called embodiment hypothesis: the premise that a machine cannot attain human-level intelligence without having some kind of body that interacts with the world.26 In this view, a computer sitting on a desk, or even a disembodied brain growing in a vat, could never attain the concepts necessary for general intelligence. Instead, only the right kind of machine—one that is embodied and active in the world—would have human-level intelligence in its reach. (Location 4276)


> Hacking aside, another problem will be what we might call human nature. People will inevitably want to play pranks on fully autonomous self-driving cars, to probe their weaknesses—for example, by stepping on and off a curb (pretending to be about to cross the street) to keep the car from moving forward. How should cars be programmed to recognize and deal with such behavior? (Location 4341)


> I believe that it is possible, in principle, for a computer to be creative. But I also believe that being creative entails being able to understand and judge what one has created. In this sense of creativity, no existing computer can be said to be creative. (Location 4394)


> Computers started off as human. In fact, they were usually women who performed calculations by hand or with mechanical desk calculators, such as the calculations needed during World War II to compute missile trajectories to help soldiers aim their artillery guns. This was the original meaning of computer. According to Claire Evans’s book Broad Band, in the 1930s and ’40s, “the term ‘girl’ was used interchangeably with ‘computer.’ One member of the … National Defense Research Committee … ballparked a unit of ‘kilogirl’ energy as being equivalent to roughly a thousand hours of computing labor.”14 (Location 4451)


> What we do know is that general human-level AI will require abilities that AI researchers have been struggling for decades to understand and reproduce—commonsense knowledge, abstraction, and analogy, among others—but these abilities have proven to be profoundly elusive. Other major questions remain: Will general AI require consciousness? Having a sense of self? Feeling emotions? Possessing a survival instinct and fear of death? Having a body? As I quoted Marvin Minsky earlier, “This is still a formative period for our ideas about mind.” I (Location 4470)


> It’s often hard to predict in what circumstances an AI system’s brittleness will come to light. In transcribing speech, translating between languages, describing the content of photos, driving in a crowded city—if robust performance is critical, then humans are still needed in the loop. I think the most worrisome aspect of AI systems in the short term is that we will give them too much autonomy without being fully aware of their limitations and vulnerabilities. We tend to anthropomorphize AI systems: we impute human qualities to them and end up overestimating the extent to which these systems can actually be fully trusted. (Location 4514)


> The economist Sendhil Mullainathan, in writing about the dangers of AI, cited the long-tail phenomenon (which I described in chapter 6) in his notion of “tail risk”: We should be afraid. Not of intelligent machines. But of machines making decisions that they do not have the intelligence to make. I am far more afraid of machine stupidity than of machine intelligence. Machine stupidity creates a tail risk. Machines can make many many good decisions and then one day fail spectacularly on a tail event that did not appear in their training data. This is the difference between specific and general intelligence.20 (Location 4519)


> Or as the AI researcher Pedro Domingos so memorably put it, “People worry that computers will get too smart and take over the world, but the real problem is that they’re too stupid and they’ve already taken over the world.”21 (Location 4526)

