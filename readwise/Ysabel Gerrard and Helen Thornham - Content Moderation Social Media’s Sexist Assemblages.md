# Content Moderation: Social Media’s Sexist Assemblages

![rw-book-cover](https://journals.sagepub.com/pb-assets/cover-alt/nms-cover-social-1565280520237.jpg)

## Metadata
- Author: [[Ysabel Gerrard and Helen Thornham]]
- Full Title: Content Moderation: Social Media’s Sexist Assemblages
- Category: #articles
- Document Tags: [[harmful_content]] 
- URL: https://journals.sagepub.com/doi/10.1177/1461444820912540

## Highlights

> Content moderation – ‘the organized practice of screening user generated content (UGC) posted to Internet sites, social media and other online outlets, in order to determine the appropriateness of the content for a given site, locality, or jurisdiction’ ([Roberts, 2017a](https://journals.sagepub.com/doi/10.1177/1461444820912540#bibr63-1461444820912540): 44) – has gained increased scholarly and public attention in recent years. ([View Highlight](https://read.readwise.io/read/01h7hwbkkd2n28jzx2jqp3z6dw))


> Researchers have so far focussed on the human labour behind content moderation ([Carmi, 2019](https://journals.sagepub.com/doi/10.1177/1461444820912540#bibr10-1461444820912540); [Roberts, 2016](https://journals.sagepub.com/doi/10.1177/1461444820912540#bibr62-1461444820912540), [2017b](https://journals.sagepub.com/doi/10.1177/1461444820912540#bibr64-1461444820912540), [2019](https://journals.sagepub.com/doi/10.1177/1461444820912540#bibr10-1461444820912540)), social media platforms’ changing responsibilities ([Gillespie, 2015](https://journals.sagepub.com/doi/10.1177/1461444820912540#bibr35-1461444820912540), [2018](https://journals.sagepub.com/doi/10.1177/1461444820912540#bibr36-1461444820912540); [Suzor, 2019](https://journals.sagepub.com/doi/10.1177/1461444820912540#bibr73-1461444820912540)), users’ experiences of platforms’ interventions ([Duguay et al., 2018](https://journals.sagepub.com/doi/10.1177/1461444820912540#bibr24-1461444820912540); [Gerrard, 2018](https://journals.sagepub.com/doi/10.1177/1461444820912540#bibr28-1461444820912540); [Myers-West, 2018](https://journals.sagepub.com/doi/10.1177/1461444820912540#bibr55-1461444820912540)) and community-driven forms of moderation ([Lo, 2018](https://journals.sagepub.com/doi/10.1177/1461444820912540#bibr52-1461444820912540); [Seering et al., 2019](https://journals.sagepub.com/doi/10.1177/1461444820912540#bibr68-1461444820912540); [Squirrell, 2019](https://journals.sagepub.com/doi/10.1177/1461444820912540#bibr69-1461444820912540)). Uniting this research is a focus on humans and machines, partly through the legacy of Science and Technology Studies (STS) scholarship (e.g. [Barad, 2009](https://journals.sagepub.com/doi/10.1177/1461444820912540#bibr3-1461444820912540); [Suchman, 2007](https://journals.sagepub.com/doi/10.1177/1461444820912540#bibr70-1461444820912540); [Wajcman, 1991](https://journals.sagepub.com/doi/10.1177/1461444820912540#bibr81-1461444820912540)) and also because of the increasing need to understand how social norms ‘leak across’, to use [Cheney-Lippold’s (2017](https://journals.sagepub.com/doi/10.1177/1461444820912540#bibr12-1461444820912540): 143) term, to content moderation processes and vice versa. A fundamental yet academically under-addressed part of content moderation debates is *gender*; specifically, how gender norms factor into and get reproduced by content moderation processes and outcomes. ([View Highlight](https://read.readwise.io/read/01h7hwgpm1xw5dad7xdredxv25))


> This leads us to our proposition of *sexist* assemblages. Assemblages, as [Wiley et al. (2012)](https://journals.sagepub.com/doi/10.1177/1461444820912540#bibr82-1461444820912540) note, *do* something, and we argue that only by understanding the particular arrangement of social media content moderation’s many elements can we see how they perpetuate rigid gender roles, typically about women; in short, how they perpetuate *sexism*. ([View Highlight](https://read.readwise.io/read/01h7hwyx388yyj79zprhddxxxy))


> At present, there are two dominant forms of social media content moderation: (1) automated and (2) human. Automated content moderation relies on machine learning techniques which ‘consistently maps onto existing data’ ([Thornham, 2018](https://journals.sagepub.com/doi/10.1177/1461444820912540#bibr74-1461444820912540): 17) in a ‘recursive loop’ ([Day and Lury, 2016](https://journals.sagepub.com/doi/10.1177/1461444820912540#bibr17-1461444820912540): 43): it matches content against known data and databases of ‘unwanted’ ([Roberts, 2017b](https://journals.sagepub.com/doi/10.1177/1461444820912540#bibr64-1461444820912540): n.p.) or flagged content, measuring the distance between points within systems and between certain words or images ([Sumpter, 2018](https://journals.sagepub.com/doi/10.1177/1461444820912540#bibr71-1461444820912540): 198). Automated moderation encapsulates the processes of uploading content and the period after: they are both pre-emptive *and* retrospective ([Gillespie, 2018](https://journals.sagepub.com/doi/10.1177/1461444820912540#bibr36-1461444820912540)). Machine learning moderation compares content with existing data, which means unique content needs to be already normative, or at least ‘known’ for machine learning moderation to ‘see’ it as a constitutive element to prompt action, such as deletion. This has a number of implications, but for the purposes of this article also demonstrates why human content moderation is so important for setting the parameters of normativity from which the automated systems can learn and build. ([View Highlight](https://read.readwise.io/read/01h7hx1ckc7vhkvdfrjec3kvjp))


> When content is flagged, it is often redirected to a human commercial content moderator (CCM) who is given ‘seconds’ ([Roberts, 2017b](https://journals.sagepub.com/doi/10.1177/1461444820912540#bibr64-1461444820912540)) to decide if it should stay or go. Content moderation is also outsourced to users who are asked to ‘flag’ inappropriate content to feed into moderation algorithms ([Crawford and Gillespie, 2016](https://journals.sagepub.com/doi/10.1177/1461444820912540#bibr14-1461444820912540)). Flagged content is weighted differently within the constitutive elements of algorithms and automated moderation learns from and develops such weighting to create different processes and re-evaluate past outcomes, such as alerts, deletions and restrictions on access to certain content ([Suzor, 2016](https://journals.sagepub.com/doi/10.1177/1461444820912540#bibr72-1461444820912540)). All of these forms of content moderation are limited for the reasons outlined above, and the automated techniques described above are famously ‘imperfect’ ([Roberts, 2017b](https://journals.sagepub.com/doi/10.1177/1461444820912540#bibr64-1461444820912540): n.p.). ([View Highlight](https://read.readwise.io/read/01h7hx25g0rzysn86nsn9hrhxy))


> Rule-setting is subjective and reflects the biases and worldviews of the rule-setters, and social media’s community guidelines are, as [Roberts (2019)](https://journals.sagepub.com/doi/10.1177/1461444820912540#bibr65-1461444820912540) reminds us, developed ‘in the specific and rarefied sociocultural context of educated, economically elite, politically libertarian, and racially monochromatic Silicon Valley, USA’ (pp. 93–94). ([View Highlight](https://read.readwise.io/read/01h7hxfcz7jf7g4h3qqjmc93sv))


> We originally collected these images to conduct a cross-platform visual analysis, an approach influenced by [Ging and Garvey’s (2017)](https://journals.sagepub.com/doi/10.1177/1461444820912540#bibr37-1461444820912540) finding that images relating to mental health on Instagram are highly aestheticised. Using a clean browser and a new account, we wanted to see what the platforms showed us – a form of platformed *show and tell* – using their search engines to discover new content, precisely as social media users would.[4](https://journals.sagepub.com/doi/10.1177/1461444820912540#fn4-1461444820912540) ([View Highlight](https://read.readwise.io/read/01h7hxqjqxt8ehhxfpesb7g8q7))


> The power and politics of social media content moderation not only lie in its processes and outcomes, but also in the decisions about *what* gets moderated and *why* this should happen. This communicative work partly takes place in social media platforms’ ‘community guidelines’, sometimes called ‘community standards’ or similar. Most, if not all platforms have these public-facing documents and they serve a unique and academically under-addressed purpose: they purport to lay out, in ‘deliberately plainspoken language’ ([Gillespie, 2018](https://journals.sagepub.com/doi/10.1177/1461444820912540#bibr36-1461444820912540): 46), how platforms want their users to behave and what kinds of content they think are and are not acceptable. ([View Highlight](https://read.readwise.io/read/01h7hygm2qhgr9xe7jjgkaqarg))


> But community guidelines are always-already inadequate as a representation of action or policy because the assemblage is in process and iterating beyond that moment. ([View Highlight](https://read.readwise.io/read/01h7hyg89p20acprs5y2njr767))


> Some ‘rules’ are more stable than others such as those against supporting terrorism, crime and hate groups, sharing sexual content involving minors, malicious speech and so on, mostly because they verge on or cross the threshold of illegality. But some of platforms’ other rules, such as those about eating disorders, are less stable and reflect morality rather than legality. ([View Highlight](https://read.readwise.io/read/01h7hyh5amsax7tyqa71yhc98h))


+++++ 
- Note: This is fascinating. Implies (or states?) that the kinds of regulations that the OSB will introduce already existed in platforms. That it won't change much in terms of social media platforms' stated guides (but may change them re how they police or moderate those guidelines).


> If, as [Gillespie (2018)](https://journals.sagepub.com/doi/10.1177/1461444820912540#bibr36-1461444820912540) notes, ‘the full-time employees of most social media platforms are overwhelmingly white, overwhelmingly male, overwhelmingly educated, overwhelmingly liberal or libertarian, and overwhelmingly technological in skill and worldview’ (p. 12), we cannot ignore the profound implications this has on their rules and the broader sexist assemblages we discuss in this article. ([View Highlight](https://read.readwise.io/read/01h7hytewcntr07exdq97mnswd))


> a further reminder of the need to consider content moderation as an *assemblage*. ([View Highlight](https://read.readwise.io/read/01h7hyyqmcfzybgd2aws6zzgxz))


> More recently, however, scholars have questioned the idea of the filter bubble through empirical research that suggests that algorithmic recommendations pay less lip service to variables such as user data and previous data trails and *more* lip service to corporate sponsors and geolocation ([Introna, 2016](https://journals.sagepub.com/doi/10.1177/1461444820912540#bibr45-1461444820912540); [Noble, 2018](https://journals.sagepub.com/doi/10.1177/1461444820912540#bibr57-1461444820912540): 5). ([View Highlight](https://read.readwise.io/read/01h7hz3bb7y4p5ad9y2mnads46))


> [Sumpter (2018)](https://journals.sagepub.com/doi/10.1177/1461444820912540#bibr71-1461444820912540) argues that one of the mathematical formulas applied to social media data is ‘principle component analysis’ (PCA).[8](https://journals.sagepub.com/doi/10.1177/1461444820912540#fn8-1461444820912540) PCA works by isolating the strongest correlations in the data and it does this by thematically collating a range of variables into ‘cleaner’ categories, partly to have fewer categories and therefore stronger correlations ([Sumpter, 2018](https://journals.sagepub.com/doi/10.1177/1461444820912540#bibr71-1461444820912540): 29–31). ([View Highlight](https://read.readwise.io/read/01h7hz64earatphcx6mqcfqvps))


+++++ 
- Note: -- this is fascinating. Kind of the opposite of what I'm thinking about for transparencies processes.


> The ‘also-liked’ algorithm then bumps up that misreading or simplification of something like gender to grossly exaggerate it as a signifier, and as it increasingly sees this variable, it notes it and gives it more weight. It is the new categories generated through this process that we are concerned with in this article, because what gets generated through recommendation systems are *over-simplified* versions of gender and other identities. To represent only over-simplified versions of gender is, we argue, a form of sexism. ([View Highlight](https://read.readwise.io/read/01h7j0spndd5x2kv81xw8jbqd0))


> It is particularly interesting to us that recommendation systems do not at first seem to be part of the content moderation process, but this is precisely their power. Recommendation systems and content moderation are not typically discussed together, and this is because they constitute content which is *left over* after moderation has taken place: what other people have ‘liked’, the accounts they might want to follow, the posts they ‘might love’. In other words, recommendations represent the most acceptable content social media has to offer ([Gerrard and Gillespie, 2019](https://journals.sagepub.com/doi/10.1177/1461444820912540#bibr29-1461444820912540)). Recommendation systems are essentially moderation systems: they are perhaps the most seemingly neutral element of social media and yet the stakes for *how* they categorise content are especially high for how we ‘see’ gender, along with race, sexual orientation, age, ability and social class. ([View Highlight](https://read.readwise.io/read/01h7j0w4nmghkh4gar68yqb9ss))


> In this article, we have presented three of the many potential elements of what we call *sexist assemblages*: the logics, processes and outcomes of social media content moderation. ([View Highlight](https://read.readwise.io/read/01h7j0x8s5b1vksvq8q76qfv6r))


> We have investigated sexist assemblages through three of several potential elements: (1) through the content presented to – or concealed from – users through in-platform searches, and which reflects dominant gendered, racialised and other norms. We argued here that, while problematic, the sexism at play is meant to ‘protect’ white women; (2) through social media platforms’ public-facing community guidelines, which lay out a given platforms’ codes of conduct and reveal biases and subjectivity in the decisions about what content to moderate, how best to do so and how these decisions are explained to users. Ultimately, we have argued that pro-ED content moderation reflects longer-standing anxieties around the out-of-control female body ([Bordo, 2003](https://journals.sagepub.com/doi/10.1177/1461444820912540#bibr6-1461444820912540)) and (3) through the content that is algorithmically recommended to users as they move through platforms. Here, we argued that the seeming neutrality of recommendation systems conceals a powerful process through which eating disorders (and other social phenomena) come to be linked to particular identities. ([View Highlight](https://read.readwise.io/read/01h7j0y6x7np5yb0pb7820d6bh))


> By the time the reader finds this article, the elements of social media content moderation that we identify might have changed, especially given how the elements that make up assemblages are ever-changing, and certain elements are more durable that others ([Latour, 1990](https://journals.sagepub.com/doi/10.1177/1461444820912540#bibr49-1461444820912540)). ([View Highlight](https://read.readwise.io/read/01h7j0ymxra0v05ffn2pjkfy1e))


> For example, the problem with researching algorithms, keywords, hashtags and other momentary stabilisations of social media content is that we can only access already-made decisions. It is very difficult to account for the silences, which is indeed an issue with assemblage theory itself. We recognise that one of the main criticisms of assemblage theory is that it only counts or sees the active elements, which creates problems for the unseen or silenced (and which feminist scholarship has long wanted to be attuned to). But assemblage theory helps to direct us to silences; to show us what the most durable elements of an assemblage are; to consider the performative capacity of assemblages when they are held together as ‘a whole’ ([DeLanda, 2006](https://journals.sagepub.com/doi/10.1177/1461444820912540#bibr18-1461444820912540): 11); to tell us what they *do* to the social world. ([View Highlight](https://read.readwise.io/read/01h7j0zyz46j11nwt8cdj7hnfk))

