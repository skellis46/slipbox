# McQuillan-2022-Resisting

![rw-book-cover](https://readwise-assets.s3.amazonaws.com/static/images/default-book-icon-4.11327a2af05a.png)

## Metadata
- Author: [[McQuillan, Dan]]
- Full Title: McQuillan-2022-Resisting
- Category: #books

## Highlights

> AI, as it is talked about in this book, is this layered and interdependent arrangement of technology, institutions and ideology. The general term we will use for this arrangement is ‘apparatus’. (Location 47)


> AI, as we know it, is a kind of computing, but it’s also a form of knowledge production, a paradigm for social organization and a political project. (Location 59)


> AI is political because it acts in the world in ways that affect the distribution of power, and its political tendencies are revealed in the ways that it sets up boundaries and separations. (Location 63)


> The theme explored throughout the text is that AI is a political technology in its material existence and in its effects. The concrete operations of AI are completely entangled with the social matrix around them, and the book argues that the consequences are politically reactionary. The net effect of applied AI, it is claimed, is to amplify existing inequalities and injustices, deepening existing divisions on the way to full-on algorithmic authoritarianism. (Location 73)


> As we look forward with trepidation to the consequences of the climate crisis, with the likelihood that privilege will be defended, responsibility deflected and the vulnerable sacrificed, our priority for advanced technologies like AI should be to ask not only how they can be prevented from intensifying harm but how we can reassert the primacy of the common good. (Location 108)


> AI’s potential contribution is as a vector for normalizing specific kinds of responses to social instabilities. (Location 121)


> we can refer to a widely used, if somewhat condensed, summary of fascism that describes it as ‘palingenetic ultranationalism’ (Griffin, 1993). These two words distill the ideology into features that are constant over time, and helps us to avoid getting diverted into looking for exact repeats of fascist rhetoric from the 1930s. The palingenetic bit simply means national rebirth; that the nation needs to be reborn from some kind of current decadence and reclaim its glorious past, a process which will inevitably be violent. The term ultranationalism indicates that we’re not talking about a nation defined by citizenship but by organic membership of an ethnic community. (Location 123)


> Hence, with AI, we should be watchful for functionality that contributes to violent separations of ‘us and them’, especially those that seem to essentialize differences. (Location 128)


> In terms of the political and social conditions, what is required to trigger a turn to fascism is a deep social crisis of some kind. The extremist ideas of fascism only start to have mass appeal when there’s a sense of existential risk. For a crisis to be ‘fascisminducing’ or ‘fascism-producing’ (Eley, 2016, cited in Malm and The Zetkin Collective, 2021) it has to appear to be beyond the capacity of traditional systems to solve. But this is only one side of the equation; the other is the decision of the dominant social class to invoke fascistic forces as a way to preserve their existing power. (Location 130)


> So, as far as AI is concerned, we need to be aware of both dynamics: the forms of crisis under which AI emerges and for which it is seen as a potential solution, and the aspirations of elites to use AI as a way to maintain existing political and cultural privilege. (Location 136)


> So, the starting point for an anti-fascist approach to AI is an alertness to its operation as a technology of division, to its promotion as a solution for social crisis, and to its use to prop up power and privilege. (Location 138)


> boundaries are always constructed and what matters most is the forms of relationality that are at work in constructing those boundaries. One of the most toxic tendencies of socially applied AI is to naturalize and essentialize structural differences as part of an ‘us and them’ politics of inequality. Looking at AI from this different perspective allows us to understand it as an apparatus that helps produce aspects of the world through the exclusions it sets up, and suggests ways that we can interrupt this through horizontal forms of intervention. (Location 156)


> One reason to have a close look at the actual operations of AI is to debunk the association between it and anything we would recognize as human intelligence. Part of the problem with AI is the way the rhetorical and cultural force of the term ‘artificial intelligence’ gets used to legitimate changes to social relations; seeing AI as nothing more than elaborate statistical guesswork goes some way towards making those changes more open to question. (Location 189)


> Part of the problem with AI is the way the rhetorical and cultural (Location 190)


> So, while the focus of this chapter is on how AI actually works, we will see a tendency for it to propagate patterns of carelessness and extractiveness alongside a concentration and centralization of power. (Location 195)


> Machine learning is distinguished from traditional programming by the fact that, instead of a programmer specifying the sequence of operations which produce the desired result, machine learning algorithms are fed a sample of the required results and use statistical estimation to figure out how to reproduce them. (Location 199)


> It isn’t what most people would intuitively understand as ‘learning’: rather than the assimilation of novel concepts based on accumulated experience and common sense, machine learning is a set of mathematical operations of iteration and optimization. (Location 208)


> While machine learning has some clever mathematical tricks up its sleeve, it’s important to grasp that it is a brute force mathematical process. There’s no actual intelligence in artificial intelligence. (Location 210)


> It turns out that certain kinds of machine learning, when given enough training data and when running on powerful enough computers, can leverage numerical operations into an uncanny emulation of various human capacities, such as the ability to identify faces or to play strategy board games like Go. Of course, the computer is not ‘recognizing’ faces because it has no idea what the meaning of a face is, nor is it actually ‘playing’ anything, but even a decent imitation of these capacities by a dumb machine is impressive, and has certainly contributed to the sense of there having been a profound breakthrough in the quest for truly intelligent machines. (Location 213)


> One of the most important aspects of machine learning is not that it heralds the sudden spark of consciousness in silicon but that it is a set of computational methods with political implications. (Location 219)


> This is well illustrated by the paradigmatic deep learning dataset called ImageNet, which consists of more than 14 million labelled images, each of which is tagged as belonging to one of more than 20,000 categories, or classes. The assumption that drove the creation of the dataset was of an unambiguous labelling; a set of terms that would describe an image correctly, and which would apply to any and all instances where that image crops up in the world. In this one sweeping gesture, ImageNet amputated the idea of a standpoint and asserted the irrelevance of context or embodied experience. (Location 233)


> This carelessness towards perspective and standpoint also applies to the labour of labelling these images. The only realistic way to create a database on this scale is to use crowdsourcing, and ImageNet images were labelled by the low paid, outsourced platform workers of Amazon’s Mechanical Turk. Yet nothing of the contribution of these workers is acknowledged or granted any agency; rather they are characterized, where they are mentioned at all, as interchangeable sets of eyeballs. Anything that might identify them as having situated experience that would affect the way they label the images is ignored in favour of constructing an objectivist and universal formulation of vision at the cheapest possible cost (Denton et al, 2021). (Location 239)


> The only realistic way to create a database on this scale is to use crowdsourcing, (Location 240)


> The unrelenting demand for ever greater quantities of training data has sent existing mechanisms of data capture into overdrive. Rather than accounting for the underlying assumptions about the elements of the world being datafied, the fixity of those elements over time and the robustness of their relationships, or the inevitable slippage between labels and their objects, the solution touted for fixing any problematic outcomes from the algorithms is to collect even more data. (Location 248)


> Even when they’re not missing some important range of real-world occurrences, the datasets of deep learning are dangerously reductive. They enforce a false equivalence between data point and label, which reverberates through the machine learning models built on top of them, because these latent simplifications overlap with correspondingly reductive social models. (Location 254)


> More and more people began to believe that the field could make significant progress simply by scaling up datasets’(Dotan and Milli, 2020). But this in itself creates barriers for entry, with implications for who gets to use AI and for what purposes. A dependency on large datasets further shifts the balance of AI power to entities with the capacity to collect and process massive quantities of data. Whatever we think of specific AI applications, accepting AI means we are implicitly signing up for an environment of pervasive data surveillance and centralized control. (Location 259)


> The essential components of a machine learning system are a way to calculate the difference between its prediction and the training data (known as the loss function) and a way to iteratively improve on it (the optimizer). (Location 264)


> The role of the optimizer is to iterate repeatedly over the training data until it has minimized the loss function. When the loss function has been minimized, the machine learning system is considered to be trained; it now has a model for how to transform input data into classifications which can be interpreted as predictions. (Location 266)


> A large part of the technical effort in machine learning is devoted to getting the most accurate results from the minimization of the loss function. What is less examined is what might be lost from sight by orienting our institutions around these kinds of algorithms. (Location 272)


> Machine learning embeds the idea that the way to solve a problem is to find an objective to optimize on. Optimization is a particular kind of rationality, one that requires the context to be datafied and asserts that condensing its complexity into a calculation provides a superior kind of solution. Machine learning’s optimizations are a kind of abstract utilitarianism, a mode of calculative ordering that results in particular ways of structuring systems. The logic of optimization, which has deep Cold War roots, already underpins our systems of logistics and planning, and the combination of granular data and machine learning opens up the opportunity for it to be used for social problems. The new era of machine learning means that a similar overarching logic to that which revolutionized global supply chains, through the abstraction and datafication made possible by containerization, can now be applied directly to everyday life. (Location 273)


> While there are different kinds of machine learning algorithms, such as decision trees and support vector machines, they mostly need to be carefully tuned to get the best results. In particular, the analyst has to choose the right set of input features for the algorithm to use in its optimization, a process known as feature engineering. There are many problems where even careful feature engineering seems to lead to defeat, especially in areas like visual perception, facial recognition, and text or language comprehension. As much of an art as a science, effective feature engineering requires some element of domain expertise, that is, some grounded knowledge of the area to which the algorithm is being applied. (Location 281)


> The original aim of artificial neural networks was to emulate learning in the brain, which was understood to come from a progressive strengthening of patterns of connections between neurons. This model of how the brain learns was pithily paraphrased as “cells that fire together, wire together”. (Location 295)


> With deep learning networks, you don’t need to worry about which features of the training data to use, or whether you understand the nuances of the context, you just need to force enough training data through the layers and apply a method of optimization called stochastic gradient descent (of which more in a moment). Deep learning can find patterns in data that we can’t even put into words – the kinds of patterns that have always been intractable to analytical description. (Location 324)


> The first step in making the world available to a neural network is to encode the input data as a vector or a tensor. A vector is simply a column of numbers where each element represents an input feature. Tensors are expansions of vectors from two into three (or more) dimensions. (Location 334)


> The long history of statistical reasoning shows how state and non-state institutions have always used statistical methods to turn the diversity of lived experience into a single space of equivalence, ready for distant decision-making (Desrosières, 2010). The statistical transformations of AI are the latest iteration in this process of rendering the world ready for algorithmic governance. (Location 343)


> It’s immaterial to a neural network whether the data passing through it represents the corpus of Shakespeare’s plays or a week’s worth of traffic flow in London – it’s a set of numbers that must be mathematically traded against each other as the network tries to minimize its loss function. (Location 349)


> If your eyes glaze over somewhat when trying to visualize all these processes, don’t worry; deep learning is a complex set of nested mathematical operations that are off the scale in terms of anything we can grasp directly. All we’re trying to do here is get a bit of a handle on the inner reasoning of neural networks so that we can assess the legitimacy of applying them to different kinds of problems. The way a neural network uses backpropagation and the loss function to ‘reason’ its way to an optimal solution is known as stochastic gradient descent: if, overall, the loss is represented as points on a landscape, then gradient descent can be visualized as the network inching its way down the slope of the abstract loss landscape in small random steps, as it seeks the bottom of a valley that represents the minimum loss. (Location 367)


> In deep learning’s forward–backward sweep of prediction– correction, it seems like the process of weaving back and forth has followed the history of programmable systems, from the first Jacquard weaving looms of the early nineteenth century, which were controlled by punched cards, to the deep learning systems of the twenty-first century. Given the number of weights to be minimized and the repetitive passing back and forth, it’s obvious that backpropagation is complex and must be computationally demanding, but it is not a black box process; we can examine the values of the weights at any stage. (Location 374)


> The network can’t tell us why a particular pattern in any layer is significant: it delivers a prediction, not an explanation. So, while neural networks can extract predictions from messy input data with uncanny effectiveness, they paradoxically cast a long shadow over our chances of understanding any trade-offs they make in the process. (Location 379)


> As anyone who owns a gaming PC will know, GPU chips draw even more power than regular Central Processing Units (CPUs), and cloud computing sets this up on an industrial scale: if artificial intelligence has a soundtrack, it’s the deafening whir of cooling fans in the server farms. The amount of processing power needed to train AI models (the number of actual calculations involved) is going up exponentially: between AlexNet, the image classification algorithm from 2012, and AlphaGo, the AI that beat a top-ranking player at Go in 2016, the number of computing operations required for model training went up by a factor of 300,000 (Open AI, 2018a). AI is not only a matter of computation but a significant commitment of material resources. (Location 386)


> There are always choices to be made about the number and size of layers, their types and arrangements, and other settings, like ‘learning rate’, which are to do with the optimization algorithm. These variables are collectively known as hyperparameters, and finding the most accurate model means optimizing networks with different hyperparameters to see which one performs the best. (Location 393)


> One of the latest language models at the time of writing, called GPT-3, has 175 billion weights that need to be optimized. Training its cousin, the BERT algorithm, which is used for natural language inference, has the same carbon emissions as a trans-American flight, while using a method called ‘neural architecture search’ to optimize the hyperparameters of a similar model produces the same carbon emissions as five cars over their entire lifetimes (Strubell et al, 2019). (Location 395)


> AI research is largely privatized, or at least wholly dependent on the cloud computing resources of Amazon Web Services (AWS), Google Cloud, Microsoft Azure or Alibaba Cloud. Even the CIA now depends on the cloud infrastructure of AWS (Konkel, 2016). It may in fact be that one of the attributes of AI that governing institutions find so appealing, alongside novel applications and the dream of machine intelligence, is its innate centralization and the barriers to entry it creates. The resources required to develop cutting-edge deep learning models are not only matters of environmental justice but of social power. (Location 405)


> While there is a shift towards unsupervised models, especially in natural language processing, there is still a fundamentally extractive relationship between the original human activity of data creation and its use in deep learning. Some of this need is satisfied by the free labour we unknowingly provide online, for example by tagging our friends in photos on social media, but the bulk of the work is carried out by a poorly paid and largely invisible workforce. (Location 412)


> Perhaps the dependency of AI on extractive labour practices should come as no surprise, given the much vaunted ancestry of computing in the Difference Engine and the Analytical Engine, those mechanical creations of Charles Babbage. Babbage was not only a theorist of early computing but of the early factory system – the unifying factor in both cases being the division of labour. He hailed the advance of ‘manufacture’ over mere making based on the division and analytical regulation of the work process in the factory (Babbage, 2010). (Location 429)


> In the abovementioned volume, Babbage wrote that ‘one great advantage which we derive from machinery is the check which it affords against the inattention, idleness or the dishonesty of human agents’, and he argued that trade union combination was always ‘injurious’ to the workforce. (Location 440)


> One of the reasons people get excited about AI is because of the way it can be applied to so many different challenges. However, the characteristics of abstraction and optimization that make AI so appealing also make it brittle in ways that amount to statistical callousness. (Location 455)


> The way a deep learning network learns to recognize objects, for example, is by being shown many example images of said objects. It doesn’t develop an embodied understanding of their physicality by living among them, as we do, (Location 467)


> Founding figures of deep learning acknowledge that, while AI can perform well on specific tasks, the systems ‘are often brittle outside of the narrow domain they have been trained on’(Bengio et al, 2021). (Location 473)


> Any AI in the real world is going to be faced with unexpected examples, whether it’s navigating the chaos of traffic or deciding on unique immigration applications. AI, it seems, is both powerful and fragile. It is striking that the first pedestrian killed by a self-driving car, a high-end Volvo being used by Uber as a test vehicle, was crossing the road while laboriously pushing a bicycle laden with their shopping bags. (Location 476)


> The fact that neural networks have uncanny ways of failing as well as uncanny powers is neatly captured by the existence of adversarial examples. A typical adversarial example in object recognition is a pair of images: one will be a clearly identifiable object, such as a tortoise, and the other will be the same image with a faintly perceptible speckling or noise. The AI will correctly identify the first as a tortoise and will confidently categorize the second as something utterly different, for example an AR-15 rifle (Athalye et al, 2018). Of course, the ‘noise’ that produces this effect is not random; it uses insider knowledge of the algorithm to craft a signal that messes with it by subtly altering the image so that the learned gradients of the machine learning model push it into a different category. However, no toddler presented with the altered image would make the same mistake. (Location 513)


> This is not a superficial fragility nor is it one that’s easily fixed – it goes to the heart of the system’s ‘learning’ about the world. The neural network has learned how to efficiently map labelled examples of human perception, and it seems to us that the network learns to recognize things the way that we do. But the training dataset, no matter how large, is a small cluster of the unimaginably large data space of possible input images. The system will take any of those possible inputs, even if they look to us like garbled nonsense, and confidently assign them to a ‘most likely’ category. A clever mathematical search for adversarial examples can therefore find images that are somewhat similar to our inputs but which our model will tip into an incorrect classification (Geng and Veerapaneni, 2019). The neural network isn’t really failing when it labels the tortoise as an AR-15, it’s just that it has efficiently learned how to do something in a brittle and non-adaptive way (OpenAI, 2017). (Location 520)


+++++ 
- Note: Adversarial. Brittle. Such useful words. Suchman


> Unfortunately the vectors also inherit the social assumptions embedded in statistical patterns of word use, so that, as the title of a well-known paper that analyzes these embeddings puts it when referring to the vector relationships that the authors discovered in their study, ‘Man is to Computer Programmer as Woman is to Homemaker’ (Bolukbasi et al, 2016). (Location 549)


> Technical fixes make sense to computer and data scientists because they feel legitimated by aligning with the values of engineering and the ideals of scientific objectivity. Being disciplined into a science-like view of the world makes them believe that turning justice into maths is the way to avoid slipping into dangerous subjectivity. So concepts like fairness and equal opportunities are translated into formal metrics like ‘disparate impact’ (Selbst et al, 2018), where fairness can be measured as a mathematical distribution of benefits or harms. But, in the end, the root problem is seeing the computation as the structure that needs fixing rather than the structure of society itself. (Location 569)


> Technical ingenuity becomes part of the problem rather than part of the solution. Rather than reflecting on the need to tackle the yawning chasm of structural power and discrimination that the machines are being trained on, the engineering approach is to ‘fix’ it with even more abstraction. But the operations of oppressive power can’t be mathematically waved away. The mathematical characterization of difference can be weaponized as easily as it can be ‘corrected’, and it doesn’t require a fascistic regime for this to happen. (Location 576)


> The industry has reacted to emerging unease about AI in society by reaching for the cover of ethics, and there’s been a nearly universal adoption of ethical principles by major AI companies to accompany the development of technical tools to fix the problem. Bias is presented as an invasive outsider that can be hunted down and cleansed while companies purify themselves by paying lip service to ethical philosophy. (Location 583)


> As with scientific fields like population genetics, where ethical principles ‘play key roles in eliding fundamental social and political issues’ (Reardon, 2011, cited in Green, 2020), it is seen as better for AI to focus on developing more inclusive datasets and adopting ethical frameworks than to face up to its role in structural injustice. (Location 598)


> socially applied AI has a tendency to punch down: that is, the collateral damage that comes from its statistical fragility ends up hurting the less privileged. (Location 601)


> We are not the individual subjects of AI but the inferential subjects of AI. (Location 616)


> the strength of neural networks is that you just give them the data and they figure out the rules, but the catch is that they can’t tell you what the rules are. (Location 621)


> Many public services are high-stakes and high-pressure environments scorched by years of austerity, where the worker-in-the-loop is caught in a web composed of algorithms, regulations and institutional management. The human is the moral crumple zone, ‘just as the crumple zone in a car is designed to absorb the force of impact in a crash, the human in a highly complex and automated system may become simply a component – accidentally or intentionally – that bears the brunt of the moral and legal responsibilities when the overall system malfunctions’ (Elish, 2019, p 40). (Location 632)


> Socially applied AI systems are innate purveyors of injustice not only because they operate in an unjust system but also because they are indifferent to causality. (Location 667)


> Classifications are part of the interlocking systems of meaning and control that shape our relationships with institutions and each other, and even shape the ways we think about ourselves and relate to our own embodied being (Spade, 2015), starting with the sweeping classifications of race, gender, sexuality, disability, social class and so on. When thinking about the broader impacts of AI and its associated datasets, it’s not sufficient to question the way it might be misrepresenting of our authentic selves, but to realize that it will act to reconstruct us as a particular subject that it will then act upon. (Location 693)


> The term ‘performativity’ refers to the mode of producing that which it is claimed is simply being described. (Location 707)


> In general, applied algorithms are performative in that they help to reshape the very phenomenon they are supposedly modelling. (Location 709)


> Machine learning’s performativity will be amplified through feedback because its interventions will change the very data distributions that it learns from. (Location 714)


> If AI has access to data about even the smallest interactions in our lives, it has the potential to affect these more intimate kinds of performativity. AI’s granular interventions at the level of everyday life will interact with the forms of performativity that already underpin our deeper sense of subjectivity and embodied being, for example, of our sense of being ‘attractive’, ‘trustworthy’ or ‘hardworking’. As the iterative channelling of daily life according to the solution-oriented predictions of algorithms expands into our intersubjective experiences, it will alter our sense of how we should act in order to be who we are. (Location 720)


> In other words, however sophisticated or creative AI might seem to be, its modelling is stuck in abstractions drawn from the past, and so becomes a rearrangement of the way things have been rather than a reimagining of the way things could be. (Location 734)


+++++ 
- Note: Suhchman. Future, imagining. Change. Novelty


> This innate conservatism makes AI a good fit with the broader tendency in contemporary society known as ‘tech solutionism’– the substitution of advanced technology for any serious attempt to address the structural causes of a problem. Tech solutionism looks for what tech might be to hand rather than what injustices need to be addressed. (Location 737)


> Society is being subsumed in what we might call, after Mark Fisher’s concept of Capitalist Realism, an atmosphere of ‘AI Realism’. Fisher observed that the somewhat paradoxical response to the financial crash of 2007–08 was not a widespread questioning of the capitalist system but a more entrenched belief in it. Capitalist Realism refers to a ‘widespread sense that not only is capitalism the only viable political and economic system, but also that it is now impossible even to imagine a coherent alternative to it’ (Fisher, 2009). (Location 756)


> The goal of AI is to intervene on the basis of predicted risk, so applied AI becomes an anticipatory system that, seeing a particular future, pre-empts it. (Location 763)


> AI’s solutionism selects some futures while making others impossible to even imagine. (Location 767)


> AI emulates science by collecting data and making models, but the predictions of AI diverge from scientific process; they are not the expressions of a hypothesis, a coherent theory about the way things work, but simply extrapolations from superficial patterns. (Location 785)


> Statistics is used by AI, as it is by science, but not in a way that tells us how much confidence we can have in our choice of parameters or whether they’re a good description for the underlying process. AI is not realist but instrumentalist: it only models the world to get something out of it. (Location 789)


> AI is trying to prove its worth as a paradigm in the same way that science had to. Science established itself by showing that mathematical representations of the world could supersede the predictions of our senses by providing an explanation for phenomena from the relative movement of the planets and the Sun (Copernicus) to the motion of bodies on the Earth’s surface (Galileo). (Location 797)


+++++ 
- Note: What are responsiblities of a discipline like dance? Suchman

## New highlights added April 10, 2023 at 8:38 PM

> When Newton explained refraction, for example, he replaced colour with a number (the ‘degree of refrangibility’). In a similar way, the operations of AI depend on datafication: the presentation of the world as data separated out from the continuity of experience. (Location 810)


> Unfortunately, one corollary of abstraction is the effect that philosopher of science Alfred North Whitehead criticized as ‘explaining away’: by treating abstractions as something concrete, everything that does not fit into the schema is denied the status of proper existence (Whitehead, 1997). (Location 812)


> Converting the social world into data for the benefit of AI is to convert ourselves into a standing reserve for optimization and prediction. We are abstracted and reduced to that which can be usefully optimized. (Location 827)


> AI needs to be understood not as an instrument of scientific measurement but as an apparatus that establishes ‘relations of becoming’ between subjects and representations. The subject co-emerges along with the representation. The society represented by AI is the one that it actively produces. (Location 834)


> Overall, AI perpetuates the ‘view from nowhere’. The view from nowhere is the claim to a neutral and disinterested view of the world. Science itself is the ultimate expression of this viewpoint: the scientific observer is a ‘modest witness’ (Haraway, 1997) who allows nothing subjective to interfere with the construction and observation of the experiment and the recording of the results. (Location 836)


> The scientism of AI allows alternative perspectives to be blunted or dismissed as subjective, and it reinforces the notion of representations that stand outside and above the context which they are used to pronounce judgement on. But, in practice, AI acts as an epistemic power grab that conceals politics and ideology under its machinic opacity. As we’ll see later in this chapter, and in the next, the invocation of sciencelike authority provides AI with the cover to propagate a variety of different forms of violence. (Location 844)


> in practice, AI acts as an epistemic power grab (Location 846)


> Its connectionism drives a kind of apophenia, ‘the perception of connectedness in unrelated phenomena’, along with ‘a sense of abnormal meaningfulness’ (Brugger, 2001), (Location 938)


> The merging of bureaucracy and AI is really a kind of continuity, as the discipline of statistics has been wedded to the needs of the state from its very inception. Bureaucracy, according to its most well-known theorist Max Weber, emerged as a mechanism for the state to impose control and rationality on increasingly complex societies, and AI is part of the same quest for traction on an increasingly complex and turbulent world. AI’s predictions are the latest version of ‘seeing like a state’ (Scott, 1999). (Location 1001)


> Like AI, bureaucracy is a generalized and goal-oriented mode of rational ordering which lays claim to neutrality and objectivity. It is also justified by the idea of efficiency at scale and, like AI, introduces a fundamental opacity as part of that scaling. Even Weber himself observed that ‘bureaucratic administration always tends to exclude the public, to hide its knowledge and action from criticism as well as it can’ (Weber, 1978, p 992). Like AI, the architecture of bureaucracy deals with the world through abstract categories and the construction of distance. Bureaucracy abstracts from the detail of social life in order to extend ‘the distance at which human action is able to bring effect’ (Bauman, 1989, p 194). (Location 1006)


> One consequence of the bureaucratic nature of AI will be the scaling of what philosopher Miranda Fricker refers to as epistemic injustice. (Location 1014)


> One kind of epistemic injustice is testimonial injustice, where prejudices cause people to ‘give a deflated level of credibility to a speaker’s word’ (Fricker, 2007, p 1). (Location 1016)


> The other kind of epistemic injustice is hermeneutical injustice, ‘a kind of injustice in which someone is wronged specifically in her capacity as a knower’ (Fricker, 2007, p 20). Fricker points to this as the kind of injustice experienced by social groups who lack the resources to make sense of their own experience. AI contributes to hermeneutical injustice because the complexity and opacity of AI-driven interventions are inherent barriers to any independent effort at comparable sensemaking (van den Hoven, 2019). (Location 1019)


> The widespread application of machine learning points to a growth in learned helplessness among data subjects, who are unable to comprehend the decisions that are being made, unable to discuss them meaningfully with others and unable to effectively dispute them, thus dispensing with the core characteristics of due process and democratic accountability. (Location 1026)


> Epistemic violence is a term used by post-colonial theorists to describe the way the experiences of people in former colonies only become known through knowledge created by the distant colonial centre, in a process that constitutes them as the inferior and problematic Other (Spivak, 1988). (Location 1033)


> Under these conditions, AI will produce the kind of thoughtlessness that Hannah Arendt warned us about (Arendt, 2006). Thoughtlessness manifests as the inability to critique instructions, the lack of reflection on consequences, and a commitment to the belief that the correct ordering is being carried out. It is the psychopolitical product of a certain kind of apparatus – of a certain arrangement of ways of knowing, cultural values and institutional arrangements. Thoughtlessness enables participants to evade any responsibility for wider harms. (Location 1038)


> Arendt also wrote about what she saw as a crisis arising from the split between the knowledge generated by technical ways of knowing and the ability to discuss those truths in normal speech and thought. For Arendt, this also contributed to the generation of thoughtlessness because, if ‘knowledge and thought have parted company for good ... we would indeed become the helpless slaves, not so much of our machines as of our know-how, thoughtless creatures at the mercy of every gadget which is technically possible, no matter how murderous it is’ (Arendt quoted in Schiff, 2013, p 104). (Location 1048)


> As much as ‘democracy bears the colony within it’ (Mbembe and Corcoran, 2019, p 27), then so does deep learning. (Location 1083)


> The colonial system and the slave system are ‘democracy’s bitter sediment’(Mbembe and Corcoran, 2019, p 27), (Location 1085)


> According to philosopher Giorgio Agamben, our norms and rights are themselves rooted in the state of exception because they are constitutional and depend in turn on the constituent power. We are living in a kind of fiction, an existing state of emergency from which we cannot return directly to the state of law ‘for at issue now are the very concepts of state and law’ (Agamben, 2005, p 87). (Location 1268)


> Applications of AI fit Agamben’s criteria by being able to enforce exclusion while remaining opaque and outside discourse or regulation. People’s lives can be impacted simply by crossing some statistical confidence limit, and they may not even know it. AI’s actions of segregating and scarcifying can have the force of the law without being of the law, and will create what we might call ‘algorithmic states of exception’. (Location 1274)


> Governments are already implementing fully fledged states of exception for refugees and asylum seekers. Agamben uses the term ‘bare life’to describe the body under the state of exception, stripped of political or civil existence. (Location 1297)


> ‘Racism, specifically, is the state-sanctioned or extralegal production and exploitation of group-differentiated vulnerability to premature death’ (Gilmore, 2006, p 28). (Location 1452)


> Core work in the creation of statistical methods as we now know them was being conducted alongside the pursuit of a racial imperialist view of national progress. (Location 1483)


> The general assumption of AGI believers is that mind is the same as intelligence, which is itself understood as logic and rationality. (Location 1530)


> that we need to be very wary of where the systematic application of discriminative ordering can end up. The necropolitical tendencies that we’ve outlined in AI resonate with the contemporary turn to farright politics. This form of politics is re-emerging in (Location 1568)


> The first layer of reactionary politics that forms a visible penumbra around the AI industry can be loosely referred to as ‘ultrarationalism’ because its most identifiable characteristic is a sociopathic commitment to statistical rationality. (Location 1579)


> One of the trademarks of tech-style rationalism is a frequent reference to Bayesianism. Bayesian statistics, which is widely used in machine learning, is an interpretation of probability that doesn’t focus on frequency of occurrence (the basis of classical statistics) but on expectations representing a prior state of knowledge. The relevant thing here is that Bayesian statistics reflects the state of knowledge about a system and is modified by ‘updating your priors’(factoring in new or updated knowledge). Ultrarationalists believe Bayesianism provides a superior approach to any problem compared to actual expertise or lived experience (Harper and Graham, nd). Enthusiasts pride themselves on adopting it not only as an approach to designing machine learning algorithms but as a rational and empirical way of tackling everyday life, without being diverted by anything as misleading as emotion or empathy. (Location 1582)


> This kind of ultrarationalism and its entanglements with artificial intelligence were initially articulated on blogs such as LessWrong, whose progenitor was the self-styled theorist of superintelligent AI, Eliezer Yudkowsky, and on blogs like the ultrarationalist touchstone Slate Star Codex. (Location 1592)


> As noted by Elizabeth Sandifer, a researcher and writer who has studied the ultrarationalists in depth, the standpoint of these blogs resonates strongly with the tech sector because both communities see themselves as iconoclastic, fearlessly overturning established knowledge using only the power of their own clever minds. ‘It is no surprise that this has caught on among the tech industry. The tech industry loves disruptors and disruptive thought,’ she says, ‘But … [t]he contrarian nature of these ideas makes them appealing to people who maybe don’t think enough about the consequences’ (Metz, 2021). (Location 1596)


> Neoreaction, or what one of its founding thinkers, Nick Land, calls ‘the Dark Enlightenment’ (Land, 2012), is an ideology that embraces and amplifies concepts like data-driven eugenics. It draws from strands of thinking that, like the alt-right and new-wave White supremacy, have their wellspring in online forums and discourse. One thing that distinguishes neoreaction from some of the other manifestations of the online far right, like the frothing misogyny of Gamergate (the online harassment of women and feminism in the game industry) or hate-trolling of 8chan (a message board site with links to White supremacism), is its relative coherence as an ideology. And while neoreaction as a movement may have limited reach, the currents it pulls together are significant because of their alignment with the affordances of AI. In fact, neoreaction can be situated as the theoretical wing of AI-driven necropolitics. (Location 1613)


> Neoreaction is the ascendency of capitalist technocracy without the trappings of electoral legitimacy, and with an almost mystical belief in authority and hierarchy. (Location 1632)


> One of neoreaction’s most prolific interpreters, Curtis Yarvin (aka Mencius Moldbug), calls this neocameralism, a reference to his admiration for the political and bureaucratic system of Frederick the Great of Prussia. The future nation doesn’t have citizens but shareholders: ‘To a neocameralist, a state is a business which owns a country’ (Moldbug, 2007). (Location 1634)


> Attempts to stop AI’s emergence, moreover, will be futile. The imperatives of competition, whether between firms or states, mean that whatever is technologically feasible is likely to be deployed sooner or later, regardless of political intentions or moral concerns. These are less decisions that are made than things which happen due to irresistible structural dynamics, beyond good and evil. (MacDougald, 2015) (Location 1641)


> Neoreaction seems to manifest a pure form of the kind of thoughtlessness that already goes with AI, and a lack of emotional engagement carried to the point of pathology. (Location 1647)


> In Schmitt’s terms, ‘Every actual democracy rests on the principle that not only are equals equal but unequals will not be treated equally. Democracy requires, therefore, first homogeneity and second – if the need arises – elimination or eradication of heterogeneity’(Schmitt, 1988, p 9). (Location 1668)


> Given the historical alliances between fascism and big business, we should also ask whether contemporary AI corporations would baulk at putting the levers of mass correlation at the disposal of regimes of rationalized ethnocentrism. (Location 1678)


> AI is technosocial solutionism, while fascism is ultranationalistic solutionism. (Location 1699)


> Fascism is less a coherent ideological proposition than a set of ‘mobilising passions’ (Paxton, 2005), at the root of which is a passionate polarization, a struggle between the pure and the corrupt, where one’s own ethnic community has become the victim of unassimilable minorities. (Location 1704)


> According to Hassabis, an exponential improvement in AI is far more likely to save the world from the climate disaster than changes in human behaviour (Heath, 2018). We’ve already noted, however, some of the contributions of AI to climate change, such as its reckless energy demands, its consumption of water resources, and its promotion as an accelerant for the fossil fuel industry. AI is already a seamless part of a wider system where extraction follows closely on the heels of abstraction, where everything of the world is seen as a utilitarian resource, not as a component of a fragile ecosystem. (Location 1712)


> The primary climate threat posed by AI is not the egregious use of energy to train the models but the idea that AI is key to ‘solving’ climate change. (Location 1717)


> As extensively documented in the book White Skin, Black Fuel (Malm and The Zetkin Collective, 2021), there is a long and ignoble alliance between elites committed to fossil fuel extraction and far-right political movements, which is expressed both by funding and by ideological overlap. (Location 1727)


> Ecofascism typically blames overpopulation in the Global South for both global warming and for the flow of refugees and migrants into western countries. An example of this can be seen in the recent switch of France’s far right Rassemblement National (formerly the National Front) from climate denialism to a position of blaming ‘planetary nomadism’ for destroying ‘European ecological civilisation’ (Malm and The Zetkin Collective, 2021, p 136). The primary tool of this Malthusian extremism is the border: ‘It is by returning to the borders that we will save the planet’ and ‘The best ally of ecology is the border’ (Malm and The Zetkin Collective, 2021, p 136). The dividing practices of AI will also find their place within ecofascism’s offer to solve the climate crisis once and for all. (Location 1738)


> As philosopher Isabelle Stengers notes in her commentary on the climate crisis, ‘humanity is rapidly moving into a state of global apartheid, one organised around questions of security and access to resources’ (Beuret, 2017). (Location 1747)


> Successful opposition will be grounded on principles that are radically different to, but as powerful as, the ideologies of hierarchy and exclusion that dominate our epistemological frameworks. As AI itself shows so clearly, how we come to know things shapes how we come to act. (Location 1754)


> AI is an apparatus that helps to configure reality through specific arrangements of power. The perspectives that it reinforces are fundamentally unaccountable, and limit our options for being and becoming. The path to undoing AI’s violence and necropolitics starts with standpoints that are situated and relational. In this chapter we aim to transcend algorithmic exclusions through a perspective of care and mutuality, and to develop this as a basis for an alternative political praxis. (Location 1765)


> Fortunately, a lot of the hard work has already been done by thinkers who have challenged science itself from feminist and post-colonial perspectives. In particular, we can make use of the approach to science that came out of standpoint theory. (Location 1770)


> Standpoint theory is an alternative to the dominant abstractions of thought, in particular those based on a detachment from the devalued activities of care (Hartsock, 1983, cited in Puig de la Bellacasa, 2017). It is, first and foremost, a challenge to the aura of absolute objectivity that places the scientific method above other ways of knowing. (Location 1774)


> Standpoint theory is concerned with the ways that the assumptions, discursive frameworks and conceptual schemes generated by dominant groups get hard coded into the ways the rest of us think about the natural world and about social relations. It’s not saying that science just makes things up but that any particular form of science is modulated by the social order in which it develops. (Location 1783)


> If all knowledge is historically and socially situated, then dispensing with the claims to neutrality that hide its social history actually makes it more objective by making it accountable and by removing the pretence of disembodied superiority. A standpoint approach to science argues that ‘starting thought from marginalised lives’(Harding, 1998, p 18) actually provides a more rigorous way of maximizing objectivity than any optimization function can ever do. (Location 1794)


> To overcome unexamined assumptions and privilege, feminist science calls for the expansion of the scientific methodology to include ‘locating the origin of the problematics’, ‘uncovering the purpose of the inquiry’ and ‘establishing a relationship between the inquirer and their subject of inquiry’ (Roy, 2004). (Location 1799)


> Perhaps most radically for machine learning, a feminist methodology of science establishes a relationship between the researcher and the research subject. (Location 1804)


+++++ 
- Note: Critical for my presentation on Somatics. That i must do this as best as possible.


> Applying the principles of feminist science to AI is a way to counter the kind of epistemic cleansing that scours actual social relations in order to prepare them for abstraction (Puig de la Bellacasa, 2017, p 87). Feminist standpoint theory contests the way the interests of the powerful are sedimented into knowledge structures, whether that’s in orthodox science or machine learning. As Sandra Harding, one of the key theorists of this approach, said: ‘a standpoint is not the same as a viewpoint or a perspective, for it requires both science and a political struggle’ (Harding, 1998, p 150). (Location 1808)


> If we’re going to apply standpoint theory to AI we need forms of practice that counter its scientistic narrowness by centring minoritized voices. For this, we can borrow directly from post-normal science. This was proposed in the 1990s by Silvio Funtowicz and Jerry Ravetz as a way of positioning science within the wider matrix of social factors, especially when ‘facts are uncertain, values in dispute, stakes high and decisions urgent’(Funtowicz and Ravetz, 1993). They recognized that the orthodox scientific method couldn’t provide enough guidance when dealing with phenomena that were both novel and complex, and whose impacts extended in scale, time and severity. Their focus on situations with uncertain facts, disputed values and an urgent need for decision-making maps exactly onto the crisis-driven context of contemporary AI. (Location 1814)


> the big problems of human togetherness are not tractable to the narrow scientistic methodology of AI. (Location 1828)


> Rather than relying on the traditional scientific virtues of certainty and neutrality, the key axes of post-normal science become ‘uncertainty’ and ‘values’. Where situations combine high uncertainty and high decision stakes, post-normal science proposes that the usual scientific domination of ‘hard facts’ over so-called ‘soft values’ has been inverted, and values are actually the stronger variables. (Location 1829)


> The post-normal framework has a proposal for how to do this: the extended peer community. Whereas peer review by members of the scientific community is key to the legitimacy of science, and has become the gold standard for all forms of academic knowledge, the post-normal situation requires this to be democratized beyond the boundaries of narrow expertise. (Location 1835)


> The way to accommodate uncertainty is not statistical parameterization but an open dialogue where technical expertise takes its place at the table alongside social concerns. All those affected by an issue form an extended peer community for effective problem-solving. When applied to AI this becomes an enactment of standpoint theory, where the perspectives of those disproportionately affected become central to deciding the way forward. (Location 1840)


+++++ 
- Note: A possibility for conference Paper on somatics? But who is most affected?


> A post-normal approach opposes this vision of an ideal hierarchy that can be mathematically actualized and reasserts the role of direct democracy in the face of uncertainty. (Location 1845)


> The insight from standpoint theory is that knowledge is always partial, and not acknowledging this partiality means that knowledge is actually ‘taking sides’. (Location 1849)


> As feminist philosopher of science and technology, Donna Haraway, puts it: ‘objectivity turns out to be about particular and specific embodiment and definitely not about the false vision promising transcendence of all limits and responsibility’ (Haraway, 1988). According to Haraway and others, how we define our ideas about objectivity extend to the notion of what an object is in the first place and to the blurring of boundaries between self, technology and the world. As she pointed out when talking about the concept of situated knowledge, ‘objectivity cannot be about fixed vision when what counts as an object is precisely what world history turns out to be about’ (Haraway, 1988). (Location 1852)


+++++ 
- Note: Suchman


> The uncertainty that scientific and technical knowledge systems are always dealing with is an uncertainty around boundaries: what counts as ‘this’ and what counts as ‘that’. AI itself is fundamentally in the business of drawing boundaries, of deciding what is included and what is excluded. (Location 1857)


+++++ 
- Note: Suchman. When does this body become that body?


> If we are aiming instead for an alternative based on care and repair, it matters what we ground our knowledge on. We need a different understanding of the world, one based on caring about what fixes our ideas of matter and meaning in the first place. (Location 1863)


> In the same way that feminist science doesn’t abandon objectivity but redefines it, a new materialist approach is realist while at the same time questioning essentialist ideas about what is real. Rather than starting from a world of objects, a new materialist world view takes relationships as fundamental: instead of focusing on the fixity of static entities, the focus is shifted to the process of how things become. It’s not about attempting to overthrow the stability of our everyday experience by saying it’s all relative, but about paying attention to the ways that boundaries are fixed and how they might be arranged otherwise. (Location 1872)


> The argument of this book is that AI is best understood in the same way: not as a system that measures and represents the world, but as an apparatus that helps to produce aspects of the world through the specific exclusions it sets up. (Location 1894)


> According to Bohr, the way you measure – the specific setup of the apparatus – materially affects what you find. This is wave-particle duality: set things up one way, the electron is a wave, set things up another way, and it’s a particle. (Location 1899)


> And so with AI – it’s not a way of viewing the world but an intervention that helps to produce the world that it sees. Setting it up one way or another changes what becomes naturalized and what becomes problematized. (Location 1901)


> In Barad’s theory, a specific arrangement or apparatus effects a ‘cut’ that results in the apparent separation between subject and object, matter and meaning, cause and effect. (Location 1905)


> Another basic insight of quantum mechanics that we can apply to AI is the inseparability of ‘objects of observation’ and ‘agencies of observation’. In other words, you can’t separate observer and observed. Instead of the old classical model where an apparatus can measure the world as it is without affecting it, quantum mechanics ties them together: you can’t make an observation without affecting what you’re observing. ‘Even in the most supposedly abstract cases, the known does reflect back to us and on us. Quantum physicists know this, feminists know this, ethnographers know this’ (De Jaegher, 2019). (Location 1909)


> Critical pedagogy is a means of generating new knowledge to tackle shared problems (Freire, 2000) and, equally importantly in our case, of unlearning prior ways of knowing that inhibit the possibility of change. Learning and unlearning together in this way, refusing an absolute separation between observer and observed, is to develop a process of ‘thinking with care’ (Puig de la Bellacasa, 2017, p 59). (Location 1926)


> A post-normal pedagogy of AI requires a radically different approach to probability because the statistical conception of probability that drives current AI inhibits the development of structural alternatives. (Location 1939)


> Instead of being swamped by algorithmic positivism, it’s about making a commitment to what philosopher of science Isabelle Stengers calls ‘the possible against the probable’ (Majaca and Parisi, 2016). The probable lacks only one thing – to exist. Apart from that, it is ready to be deduced. The possible, however, is literally unpredictable, and the methods of reaching it are utterly different. (Location 1945)


> The approach proposed in this chapter is to recompose the question of AI as a ‘matter of care’ (Puig de la Bellacasa, 2017). Care acts as an epistemological corrective to AI because it directs attention to situated vulnerability and dependency: care is the opposite of algorithmic detachment and abstraction. (Location 1953)


> The computations of large-scale models operate at millions of floating point operations per second, not ‘at the pace of the toddling child or the elderly person with emphysema’ (Mellor, 1997, cited in Puig de la Bellacasa, 2017). (Location 1958)


> Where the AI industry focuses on building models that gain a few percentage points on an industry benchmark for predictive accuracy, a perspective of care asks how the result might amplify neglect. Starting from the principle of care is a counterproject to AI’s thoughtlessness and ‘view from nowhere’. (Location 1966)


> The observer can’t be completely separated from the observed: the knower and the known are co-emergent. Realizing that knowledge of the world is processual and relational means overcoming the division between knowing and caring. (Location 1985)


> Gustav Landauer, philosopher of social anarchism, recognized the transformative potential of restructured relationality when he wrote, ‘The state is a social relationship, a certain way of people relating to one another. It can be destroyed by creating new social relationships, i.e. by people relating to one another differently’ (Sakolsky, 2012). In our highly infrastructured societies, the relationships that need restructuring include those within and between our apparatuses. Rather than starting from the statistical assumption of separability, which always comes with a gradient of power, mutual aid embraces entanglement as a form of levelling. (Location 2017)


> Mutual aid transforms thoughtlessness through the practice of collective care and repair. (Location 2029)


> There are collective choices to be made about the kind of futures we want, not just the ones we’re statistically predicted to have, and solidarity is their starting point. (Location 2034)


> Commoning is the action of taking aspects of the world back into common ownership and stewardship, and of organizing through structures of mutual aid and solidarity. Commoning is both a refusal of segregation and an assertion of the common good. (Location 2042)


> The real antidote to exclusion isn’t inclusion, it’s commoning. Rather than requiring a unifying system of representation, the common space allows differences to be generative of common action. (Location 2046)


+++++ 
- Note: Usefur for somatics conference? Commoning re science and practice


> The workers’council is anti-bureaucratic and directly democratic – an assembly of members making decisions together about matters of immediate concern. It’s based on solidarity and self-activity, and the shared sense of a situation that is badly out of balance. (Location 2084)


> Adopting the horizontal form of the people’s council is to structurally oppose the kind of exclusion and exception that is driven forward by AI, and instead to generate relationality and solidarity. (Location 2136)


> The aim of a people’s council on AI is to create a circuitbreaker, where the complexities of situated knowledge can be counterposed to pre-emptive AI solutionism. A people’s council on AI can be seen as a militant version of post-normal science’s extended peer community, making a space for previously undervalued knowledge and expertise to be mobilized. (Location 2139)


> It’s important to distinguish people’s councils from watereddown forms of engagement such as citizens’ juries, which are currently deployed to deal with questions ranging from AI to climate change. (Location 2145)


> Participating in a citizen’s jury is not a form of self-determination but another way of being constructed as a subject, in this case as an ‘active citizen’. (Location 2153)


> The people’s council is a nomadic intervention in the space of neural networks and their application. (Location 2171)


> The Luddite’s stance was summed up in a threatening letter to M. Smith, Shearing Frame Holder at Hill End Yorkshire, signed by Ned Ludd, ‘clerk to the General Army of the Redressers’, which made clear their commitment to ‘put down all Machinery hurtful to Commonality’ (Binfield, 2004, p57). (Location 2204)


> The project of an anti-fascist approach to AI, especially as manifested in workers’ and people’s councils, is the inversion of states of exception by creating and defending spaces of autonomy. (Location 2324)


> Rather than relations which are established by the authority of an algorithm, the important relations are those entered into voluntarily and autonomously. The structural renewal of our infrastructural apparatus means a shift to cooperative labour, commons-based peer production, and other self-organized circuits of the social sphere. (Location 2331)


> The work of philosopher and social critic Ivan Illich is also relevant, especially his 1973 text, Tools for Conviviality. By tools, Illich was referring to something like our definition of apparatus: definitely the machinic elements, but also their surrounding institutional arrangements. The conviviality he was aiming for was that tools should enable ‘autonomous and creative’ activity rather than producing conditioned responses or automatization (Illich, [1973] 1975, p 24): (Location 2359)


> People feel joy, as opposed to mere pleasure, to the extent that their activities are creative; while the growth of tools beyond a certain point increases regimentation, dependence, exploitation, and impotence. (Illich, [1973] 1975, p 34) (Location 2363)


> The understanding of the commons used both in the previous chapter and here go beyond the ‘common pool resources’featured in Elinor Ostrom’s seminal work on the topic (Ostrom, 2009), where she focused on traditional commons like water resources, fisheries and forests. (Location 2400)


> Perhaps we already have all the computing that we need, and the future is more about recycling, salvaging and repurposing. (Location 2446)


> AI is at the cutting edge of the ongoing effort to subsume all human activity into the sphere of production and consumption, which ‘increasingly exploits the entire range of our productive capacities, our bodies and our minds, our capacities for communication, our intelligence and creativity, our affective relations with each other, and more. Life itself has been put to work’ (Hardt and Negri, 2012, p 16). (Location 2450)


> Rather than continuing AI’s endless fragmentation of productive activity into separated shards of experience, a new apparatus enables transition by supporting the recomposition of collective subjects – the autonomous form of ‘us’ that doesn’t require a ‘them’ to justify its existence because the ‘us’-ness comes from mutually constituting solidarity. (Location 2458)


> The framing of a new apparatus accepts that the diversity, variety and complexity of experience overflows representation and is therefore immune to abstraction. (Location 2460)

