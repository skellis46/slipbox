# How AI Is Learning to Identify Toxic Online Content

![rw-book-cover](https://static.scientificamerican.com/sciam/cache/file/C9A31747-FBED-41A5-8820ED507C404BB0.jpg)

## Metadata
- Author: [[Laura Hanu]]
- Full Title: How AI Is Learning to Identify Toxic Online Content
- Category: #articles
- Document Tags: [[harmful_content]] 
- URL: https://www.scientificamerican.com/article/can-ai-identify-toxic-online-content/

## Highlights

> In 2019, it was reported that Facebook moderators are at risk of [suffering from PTSD](https://www.theverge.com/2019/6/19/18681845/facebook-moderator-interviews-video-trauma-ptsd-cognizant-tampa) as a result of repeated exposure to such distressing content. Outsourcing this work to machine learning can help manage the rising volumes of harmful content, while limiting human exposure to it. Indeed, many tech giants have been [incorporating algorithms](https://www.cnet.com/news/inside-facebook-twitter-and-googles-ai-battle-over-your-social-lives/) into their content moderation for years. ([View Highlight](https://read.readwise.io/read/01h7ffmpx3zwp9r2xfvbqydrtq))


> Although there has been considerable progress on automatic detection of toxic speech, we still have a long way to go until models can capture the actual, nuanced, meaning behind our languageâ€”beyond the simple memorization of particular words or phrases. Of course, investing in better and more representative datasets would yield incremental improvements, but we must go a step further and begin to interpret data in context, a crucial part of understanding online behavior. A seemingly benign text post on social media accompanied by racist symbolism in an image or video would be easily missed if we only looked at the text. We know that lack of context can often be the cause of our own human misjudgments. If AI is to stand a chance of replacing manual effort on a large scale, it is imperative that we give our models the full picture. ([View Highlight](https://read.readwise.io/read/01h7ffthdx0nprt2mm6maps7q1))

