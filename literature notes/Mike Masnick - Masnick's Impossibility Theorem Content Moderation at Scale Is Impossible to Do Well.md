# Masnick's Impossibility Theorem: Content Moderation at Scale Is Impossible to Do Well

![rw-book-cover](https://www.techdirt.com/wp-content/themes/techdirt/assets/images/td-rect-logo-white.png)

## Metadata
- Author: [[Mike Masnick]]
- Full Title: Masnick's Impossibility Theorem: Content Moderation at Scale Is Impossible to Do Well
- Category: #articles
- URL: https://www.techdirt.com/2019/11/20/masnicks-impossibility-theorem-content-moderation-scale-is-impossible-to-do-well/

## Highlights

> I’ve argued for years that while many people like to say that content moderation is *difficult*, that’s misleading. Content moderation at scale [is impossible to do well](https://www.techdirt.com/blog/?tag=content+moderation+at+scale). Importantly, this is **not** an argument that we should throw up our hands and do nothing. Nor is it an argument that companies can’t do *better jobs* within their own content moderation efforts. But I do think there’s a huge problem in that many people — including many politicians and journalists — seem to expect that these companies not only can, but should, strive for a level of content moderation that is simply impossible to reach. ([View Highlight](https://read.readwise.io/read/01h7a32p09g6vq8qtbtknfet5f))


> And thus, throwing humility to the wind, I’d like to propose Masnick’s Impossibility Theorem, as a sort of play on Arrow’s Impossibility Theorem. **Content moderation at scale is impossible to do well**. More specifically, it will always end up frustrating very large segments of the population and will always fail to accurately represent the “proper” level of moderation of anyone. While I’m not going to go through the process of formalizing the theorem, a la Arrow’s, I’ll just note a few points on why the argument I’m making is inevitably true. ([View Highlight](https://read.readwise.io/read/01h7a338bp4xsq19kcswbfhtgy))


> Second, moderation is, inherently, a subjective practice. Despite some people’s desire to have content moderation be more scientific and objective, that’s impossible. By definition, content moderation is always going to rely on judgment calls, and many of the judgment calls will end up in gray areas where lots of people’s opinions may differ greatly. Indeed, one of the problems of content moderation that we’ve highlighted over the years is that to make good decisions you often need a tremendous amount of context, and there’s simply no way to adequately provide that at scale in a manner that actually works. That is, when doing content moderation at scale, you need to set rules, but rules leave little to no room for understanding context and applying it appropriately. And thus, you get lots of crazy edge cases that end up looking bad. ([View Highlight](https://read.readwise.io/read/01h7a350b4p3h5bh3cav6eqtqy))


> We’ve seen this directly. Last year, when we turned an entire conference of “content moderation” specialists into content moderators for an hour, we found that there were exactly zero cases where [we could get all attendees to agree](https://www.techdirt.com/articles/20180518/00271539858/there-is-no-magic-bullet-moderating-social-media-platform.shtml) on what should be done in any of the eight cases we presented. ([View Highlight](https://read.readwise.io/read/01h7a35fe046ckt9yzmk35hgf6))


> Third, people truly underestimate the impact that “scale” has on this equation. Getting 99.9% of content moderation decisions at an “acceptable” level probably works fine for situations when you’re dealing with 1,000 moderation decisions per day, but large platforms are dealing with way more than that. If you assume that there are 1 million decisions made every day, even with 99.9% “accuracy” (and, remember, there’s no such thing, given the points above), you’re still going to “miss” 1,000 calls. But 1 million is nothing. On Facebook alone a recent report noted that there are [350 million photos uploaded every single day](https://www.businessinsider.com/facebook-350-million-photos-each-day-2013-9?IR=T). And that’s just photos. If there’s a 99.9% accuracy rate, it’s still going to make “mistakes” on 350,000 images. Every. Single. Day. So, add another 350,000 mistakes the next day. And the next. And the next. And so on. ([View Highlight](https://read.readwise.io/read/01h7a369prp9dppvv2vaarkwqm))


> And, even if you could achieve such high “accuracy” and with so many mistakes, it wouldn’t be difficult for, say, a journalist to go searching and find a bunch of those mistakes — and point them out. This will often come attached to a line like “well, if a reporter can find those bad calls, why can’t Facebook?” which leaves out that Facebook DID find that other 99.9%. Obviously, these numbers are just illustrative, but the point stands that when you’re doing content moderation at scale, the scale part means that even if you’re very, very, very, very good, you will still make a ridiculous number of mistakes in absolute numbers every single day. ([View Highlight](https://read.readwise.io/read/01h7a36xmgg4re9gfj8fx4pbh6))

