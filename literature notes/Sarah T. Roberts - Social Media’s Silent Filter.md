# Social Media’s Silent Filter

![rw-book-cover](https://cdn.theatlantic.com/thumbor/9D0eLOGxjJzdJmG8Nn-oJfExIXY=/7x150:4993x2747/1200x625/media/img/mt/2017/03/RTX27Y1E/original.jpg)

## Metadata
- Author: [[Sarah T. Roberts]]
- Full Title: Social Media’s Silent Filter
- Category: #articles
- Document Tags: [[harmful_content]] 
- URL: https://www.theatlantic.com/technology/archive/2017/03/commercial-content-moderation/518796/

## Highlights

> Thus far, much of the post-election discussion of social-media companies has focused on algorithms and automated mechanisms that are often assumed to undergird most content-dissemination processes online. But algorithms are not the whole story. In fact, there is a profound human aspect to this work. I call it commercial content moderation, or CCM. ([View Highlight](https://read.readwise.io/read/01h7j14k31gcfy65kafa03kymm))


> As a researcher, I have studied this process in detail: In a matter of seconds, following pre-determined company policy, CCM workers make decisions about the appropriateness of images, video, or postings that appear on a given site— material already posted and live on the site, then flagged as inappropriate in some way by members of the user community. CCM workers engage in this vetting over and over again, sometimes thousands of times a day. ([View Highlight](https://read.readwise.io/read/01h7j1hhed7398d6gr8tep804p))


> While greater reliance upon algorithmic decision-making is typically touted as leap forward, one that could potentially streamline CCM’s decision-making, it would also eliminate the human reflection that leads to pushback, questioning, and dissent. Machines do not need to sign NDAs, and they are not able to violate them in order to talk to academics, the press, or anyone else. ([View Highlight](https://read.readwise.io/read/01h7j235rryy9eqec37b98mcwz))

